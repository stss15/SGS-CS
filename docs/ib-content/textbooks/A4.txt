A4 Machine learning

This can include, but is not limited to, reasoning, learning, perception, problem-solving,
understanding and interaction. Machine learning is a subset of artificial intelligence that

(.-Top tip!
Take the time to

appreciate the
differences between
types of machine
learning: supervised,
unsupervised,
reinforcement, deep
learning and transfer

focuses on the learning aspect of AL Tt seeks to teach computers to learn from data, identify
the patterns in that dara and make decisions based on what it has learned, with minimal

human intervention. Implementing machine learning programmatically is heavily reliant on
the mathematics of statistics, linear algebra and calculus.
Machine learning applications are being increasingly used throughout commerce, industry,

research and government. They are used for everything from market analysis to robotics; from
generative art to diagnosing medical conditions. The applications for machine learning will
only grow as the technology continues to develop.

learning. Know what
scenarios each is best
suited for, and the
typical algorithms
used in each category.
In this topic, terms
and definitions are
foundational for
answering theoretical
questions accurately.
Using terminology in

‘Within machine learning, there are many further subcategories we will consider in A4.3

an incorrect context

m artificial neural networks

will cost marks.

Machine learning approaches. These can be broadly described as:
|

supervised learning: linear regression

|

supervised learning: classification

|

unsupervised learning: clustering

B unsupervised learning: association rule
B reinforcement learning
B genetic algorithms

|

convolutional neural networks.

H Deep learning
# Neural network:
a computer algorithm
that imitates the
design of the human
brain by using a set of
interconnected nodes
for the processing and
analysing of data.

The term “deep learning” is used to imply the use of a neural network within a machine

learning algorithm. There are a variety of machine learning techniques that work perfectly
fine without the need for a neural network, so the “deep learning” term is used to distinguish

between those that do and those that do not make use of a neural network. For example, you
can refer 1o “reinforcement learning” and “deep reinforcement learning”.
A neural network is where algorithms and data structures have been constructed in
such a manner as to replicate biology’s understanding of how the brain functions: as an

interconnected network of neurons, each of which has various input connections and generates
an output on the basis of the combination of inputs.
inputs
dendrites

!

linear

function
nucleus

B Comparison of
a biological neuron with that used by artificial neural networks

Ad4.1 Machine learning fundamentals

activation
function

A more detailed examination of how neural networks function will be provided in A4.3.8
Artificial neural networks.

(;Common mistake
Deep learning is a subset of machine learning. Deep learning is not separate from machine
learning, but rather is a specific approach within it. It utilizes layers of neural networks to extract
progressively higher-level features from the input. Machine learning includes many other types of
algorithms that do not require neural networks.

B Supervised learning
4 Supervised
learning: when a
machine learning
algorithm is provided
a data set of pairs of
items, where the pair
comprises a value and
what response the
network should provide
if it sees that value. By
learning the answers
to the values given,
the network will make
generalizations to be
able to estimate the
answer when given a
previously unseen value.

Supervised learning refers to an algorithm that is trained on labelled data sets. These data sets

comprise example input values, and the correct output response that should be given if the
algorithm sees something resembling that input. Generally, the larger and better the data set,

the more accurate the results that will be produced by the supervised learning algorithm. Data
sets used by the major technology companies contain many millions of records.
Supervised learning can be used for regression and classification tasks.
A regression task is where the algorithm is predicting a numerical value for the output within
an allocated range, for example:
® A grade-prediction algorithm might take inputs of hours studied, attendance record, class
participation, scores on previous tests, hours spent on homework; and output a final
predicted grade in the range 0-100.
B A weather-forecasting algorithm might take inputs of historical temperatures for each day
over the last week, humidity, wind speed, air pressure; and output a predicted temperature
for the coming day in a given range.

# Regression: machine
learning where the
output generated should
be a numerical value.

A classification task is where the algorithm predicts which category the input item belongs

# Classification:
machine learning
where the output
generated should be a
category, chosen from
among a discrete set of
categories available.

GCommon mistake

to, for example an image recognition algorithm might input an image and seek to classify it as
either a dog or a muffin.

Confusing the goals of regression and classification
Be clear about the difference in outputs between regression and classification tasks in supervised

learning. Regression models predict a continuous output (numerical values), whereas classification
models predict categorical outputs (class labels). For example, predicting the price of a house
based on its features (like size and location) is a regression problem because price is a continuous
variable. On the other hand, determining whether an email is spam or not spam is a classification
problem because there are discrete categories (spam or not spam) to choose from.

A music genre classification algorithm may input song tempo, rhythm, pitch, instruments
used; and output the music genre as either pop, rock, hip-hop, classical, and so on.
A handwriting recognition algorithm may input an image of a character and seek to classity it
as an individual letter, number or punctuation mark.

A4 Machine learning

Hl Unsupervised learning
4 Unsupervised
learning: a method of
machine learning where
the data set does not
include the “answers”
or expected outputs for
the data provided. The
algorithm will attempt
to discover the patterns

Unsupervised learning is where the algorithm is constructed to identify patterns or structures
within its data sets without being provided with an explicit label indicating the correct output.

This may be because the nature of the data involved doesn't lend itself to having a “correct”
response paired with it, or because the algorithm is constantly learning based on user

interactions that don't have a fixed right or wrong answer. Examples include:
B Analgorithm that seeks to identify a user’s social group: The input data may consist of
social-media activity such as likes, comments and follows. The algorithm could analyse this
data to identify other users with mutual acquaintances or similar interests. Interestingly,

on its own.

this type of social-group analysis can take place without needing any content from the

# Reinforcement
learning: machine
learning by trial and
error. Based on what
it has learned at any
moment in time, the

messages or chats between the parties involved. This is why social-media companies such

as WhatsApp are perfectly happy to offer end-to-end encrypted messaging as, even without
the message content, just knowing how many messages are exchanged between each pair

of users is enough to perform social-group analysis.
B Rerail stores use unsupervised learning to find associations and correlations between

algorithm selects an
action to take in a
given environment. The
environment provides
feedback (called a
“reward”), which the

algorithm will use to
learn from and refine its
decision-making process
moving forward.

the different products that customers purchase, and identify similarities in purchasing
behaviour and preferences. The reason that so many brands run customer loyalty schemes

is it allows them to build a profile of data to match against other customers, from which
they can tailor marketing strategies.
B Media companies such as Netflix, Spotify and YouTube use unsupervised learning to
train recommendation systems to refine their suggestions to users for future watching
or listening.

M Reinforcement learning
Reinforcement learning is where the algorithm looks at its input data and decides on a
particular output, and is then informed how good or bad thart decision was after the fact.
It uses that information to refine future actions when presented with a similar situation.
Reintorcement learning can be thought of as learning from trial and error.

Some common situations where reinforcement learning is used include:
B

Gaming: Reinforcement learning algorithms can be trained to act as Al players or bots
within computer games.

B Robotics: Reinforcement learning can be used to teach a robot how to walk, pick up objects
or perform other mechanical tasks. As a subtype of robotics, autonomous self-driving cars

also make use of reinforcement learning to better and more safely navigate the complexities
of roads and traffic.
m Finance: Reinforcement learning bots can trade securities on the market and receive
feedback based on whether the bot made or lost money on the trade.
B Recommendation systems: Reinforcement learning can also be part of a suite of algorithms
used in generating user recommendations. The engagement of the user (did they watch or
listen to the suggested item?) can be used to provide feedback to the algorithm to refine

future recommendations.

Ad4.1 Machine learning fundamentals

B Transfer learning
# Transfer learning:
when a previously
trained machine learning
model is applied to
a similar yet new
situation, context or

problem. The goal is to
speed up the training
process by using an
already trained model,
even if the problem is
slightly different.

Transfer learning is where the knowledge gained from solving one problem can be used to
help solve a different but somewhat related problem. The benefit of transfer learning is that

it requires less data, as the algorithm is already partially trained and may just require a little
fine-tuning for the new task being asked of it.
Consider the following examples:
B Image recognition: Given a model that has been trained on a massive data set such as
ImageNet (over a million labelled images and 1000 different categories), transfer learning
could take that model and fine-tune it to recognize specific types of objects, such as a
species of flower or breed of dog. The model would already be adept at processing images
and easily able to identify features such as edges and shapes, so it would only need to learn
how to distinguish between the new categories.

(.-Common
mistake
Transfer learning is
not just about using
a pre-trained model.

® Speech recognition: Using a generalized model that has been trained on spoken language
to transcribe it into text, transfer learning can be used to adapt it to work with particular

accents or specialized jargon for use within a particular industry.
B Customized chatbot: By using a publicly available pre-trained LLaMA (large language
model meta AI), a company might fine-tune it by training it on customer-service logs to

It involves adapting a

create a chatbot that can be added to its website for handling domain-specific queries.

mode! developed for

® Customized image generators: Pre-trained models for tools such as Stable Diffusion can be

one task to solve a
related one; not just

reusing an existing
model without

modifications. It's

further extended and fine-tuned to generate images that mimic a particular artistic style, or
be specialized in images for a particular industry or domain. This can be done relatively
quickly and easily without the burden of redoing the massive task of original training that
went into the underlying model.

crucial where data is
scarce or similar tasks
are involved.

A4.1.2 Hardware requirements
The hardware required for machine learning purposes will continue to innovate and
evolve throughout the lifetime of this text. Accordingly, this section is not going to make
recommendations as to specific model numbers of processors, but will rather discuss the broad
categories of hardware technology available and their various use cases.

B Computing platforms
Standard laptops
The starting point is obviously the standard laptop available on the retail market. At the

time of writing, this might be an i7 processor with 16 GB or 32 GB of RAM, or an Apple
Silicon equivalent.
These machines are generally limited to small-scale machine learning tasks, such as the
development and testing of a simple machine learning model. For educational purposes,
there is a lot that can be done with a standard laptop, but you would not want to be training a
commercial-grade machine learning model with such equipment as it would be too slow, and

lack sufficient memory or storage.
Some recent developments do aim to improve the capacity of standard laptops when it comes
to machine learning. One is the introduction of Apple Silicon M processors into Apple

MacBooks. Apple integrates the CPU, GPU, neural engine and other components into a single
system-on-a-chip (SoC) structure, allowing better performance and energy efficiencies. By
integrating the CPU and GPU functions on to a single chip, they pool and share the same

A4 Machine learning

memory. This is in contrast to the traditional approach of GPUs having their own dedicated
memory, separate from the RAM used by the CPU. This is why those with an Apple Siliconbased computer are often able to perform machine learning tasks that traditional Intel laptop

owners are unable to do without access to a dedicated GPU.
Not wanting to allow Windows users to be left behind, Microsoft has launched its Microsoft

Copilot Al-supported branding, which requires laptops to have an integrated neural processing
unit (NPU), which is discussed further in the section regarding CPUs coming up.

Dedicated workstation
After a standard laptop, the next step would be the purchase of a dedicated desktop
workstation with a GPU, such as an NVIDIA RTX.
Having a true GPU can offer an order of magnitude improvement in processing speeds for
machine learning calculations and would serve as an excellent platform for some quite
sophisticated projects.
The primary advantage of a GPU is its parallel processing capabilities, which come from
having thousands of small processing cores that are optimized for parallel processing. Machine
learning algorithms often involve performing the same computations on large amounts of data.
GPUs can perform these same calculations on different values simultaneously, whereas a CPU
has to queue them up for processing one by one.

Edge devices
Ldge devices refer to computing systems that perform data processing at or near the location

where data is being generated, rather than relying on centralized computing resources such as
the cloud.
Processing data locally reduces the need to send data back and forth to a distant data
centre. This reduction in data being transmitted has the added benefit of improving privacy

and security.
The downside is that you are still committed to investing in the physical hardware

infrastructure yourself, along with all the maintenance workload associated with it.

Cloud-based platforms
To perform training on large or complex models generally requires the use of online cloudbased platforms (in lieu of investing in the massive infrastructure yourself). Cloud platforms
are accessible over the internet and provide services on demand to users worldwide.
These cloud providers allow you to vary the combination and specifications of CPUs, GPUs and
Tensor Processing Units (TPUs) available for your project on demand. They can also scale to
provide large quantities of RAM, storage and network connectivity, as required. The cloud-based
services are also useful for deployment of your model as an API for other systems to access.
The main downside with cloud-based platforms is the dependency and reliance your
project will have on an external provider. You have to trust their data and network security
arrangements; you have to transmit your data to their network to have it perform tasks for you;
and you are committing yourself to the monthly subscription costs involved. The flexibility of
cloud-based systems always comes with a cost, and this should not be treated lightly.
At the time of writing, the major industry leaders that provide cloud-based platforms with
machine learning specialist equipment available include AWS, Google Cloud and Microsoft
Azure. A good tool for getting started with minimal set-up requirements is Google Colab; it
allows you to create a Python Notebook and utilize GPU or TPU technology just by changing
the settings in the Runtime menu.

Ad4.1 Machine learning fundamentals

High-performance computing (HPC) centres
In contrast to the publicly available, user-pays approach of cloud-based providers, HPC
centres are dedicated facilities designed to support large-scale scientific or academic research

objectives. In this way, access to an HPC is more restricted, often requiring membership;
affiliation with an academic or research institution; or specific research grants or time

allocation processes.
They are data centres that have been designed to be suitable for highly demanding workloads

that require sustained high-performance computing resources. They are built around a model
of catering to resource-intensive computarional tasks, not an as-a-service model.
Many universities have made investments in their own HPCs for use by their research students.

B Processors for machine learning
Having considered the various platforms available for accessing the computing power
necessary for machine learning, it is time to review the electronics within the computers that
make machine learning happen.

Central processing units (CPUs)
CPUs are the generalized processors inside all modern computer systems. They are designed
to perform a wide range of computing operations, are highly flexible and can process complex
tasks. They are not specialized devices designed specifically for machine learning. While it is
feasible to perform some introductory machine learning tasks with a CPU, they are generally
limited to tasks that do not require intensive parallel processing.
Neural processing units (NPUs) have recently been integrated alongside rraditional CPUs
in consumer-level laptops. NPUs are specialized processors designed specifically to handle
the computations required for neural networks and deep learning, such as matrix and vector
operations. By having specialized processors in the computing device, it provides faster
processing times and lower power consumption for Al-related tasks, compared to generalpurpose CPUs.
As of 2024, laptops marketed as being Microsoft Copilot Al-supported include NPUs with a
minimum capability of 40 TOPS (trillion operations per second).

Graphics processing units (GPUs)
GPUs contain hundreds or thousands of small cores designed for highly parallel tasks such as
rendering graphics. The GPU allows all the cores to perform the same calculation on different
# Tensor: a
mathematical term for

an array with three
or more dimensions.
A single number (no

dimensions) is known
as a “scalar”. A onedimensional array of
numbers is known as
a "vector”. A two-

values simultaneously, so if there are large arrays that need processing, where every element
requires the same operation performed, GPUs provide significant time savings. GPUs excel at

parallel processing of matrix and vector operations, which is the very mathematics that forms
the basis of neural networks.
The presence of a dedicated GPU can often produce training speed improvements of up to ten
times over using just a CPU.

Tensor Processing Units (TPUs)
Building on the idea of the GPU, the TPU was custom-designed by Google specifically for

dimensional array of

tensor computations. They are optimized for high volume, low precision calculations to

numbers is known as a

increase the efficiency of neural network tasks. Low precision in this context typically means

“matrix". Three or more

calculations occur at a maximum of 16 bits, in contrast to the 32 bits or 64 bits in a normal

dimensions is known as

GPU. Machine learning generally does not require that level of precision, so 16 bits or even

a “tensor”.

8 bits will do the job.

&)

A4 Machine learning

At the heart of a TPU is a large matrix multiplication unit. Matrix multiplication is
fundamental to neural networks, so having a unit within the processor specifically optimized
for this task helps make TPUs well suited for machine learning.

The TensorFlow library is tailored to make use of TPUs when available, and Google Cloud
services, such as Google Colab, make TPUs easily available for the general public.

Application-specific integrated circuits (ASICs)
ASICs are custom-designed for a specific use rather than general-purpose computing. They

are engineered to perform a particular set of tasks with optimal efficiency. They offer peak
performance and efticiency for these tasks, but lack the general-purpose flexibility of a CPU.
1f your machine learning workload can be precisely defined and won't change much over time,
an ASIC may perform these tasks faster than a GPU or TPU as, while these are optimized for
parallelism, they are still generalized processors.

Due to the degree of specialization involved, ASICs tend to be more energy efficient and have
lower operating costs over the long term. The downside is that the uptront cost is typically
very high as the chips require custom design and development. This means they are really only
viable where a machine learning application is going to be deployed on a very large scale, as
the per-unit cost of the ASIC will decrease significantly with scale when mass-produced.
Examples of well-known, mass-produced ASICs include the Apple A-series chips used in

iPhones and Qualcomm’s Snapdragon.
You should conduct some research into the current state-of-the-art ASICs available for machine
learning operations at the time of reading, and be familiar with what differentiates them from
just using a typical GPU or TPU.

Field-programmable gate arrays (FPGAs)
FPGAs can be programmed and reprogrammed to perform specialized computing tasks,
offering a balance between the flexibility of CPUs / GPUs and the efficiency of ASICs.

As such, they are ideal for prototyping machine learning models or applications that require
custom hardware acceleration, however that may change over time.
FPGAs are used for high-frequency trading systems where microseconds can make a
significant difference in the profitability of trades.

(;Common mistake
Confusing the differences between each of the processor types
There are a lot of separate technologies listed in this topic, many of which you will not have had
personal hands-on experience with. That makes it harder to have an intuitive understanding of the
differences between them.
B ASICs are designed for specific tasks and are not reprogrammable.
FPGAs are versatile and can be reprogrammed.
GPUs are great for parallel processing tasks.
TPUs are specialized chips designed by Google, optimized for tensor calculations in deep
learning for large-scale models.

NPUs are designed to accelerate neural network computations for consumer-grade devices.

Ad4.1 Machine learning fundamentals

(;Top tip!
Adapt the following as a guide to help determine which is the
best device for a given scenario.
B For large and complete models, does it require real-time
processing?
O Yes: Consider GPUs for their parallel-processing
capabilities
0 No: TPUs might be a better choice for batch
processing with high efficiency in tensor operations
B For real-time inference (using a model for decision-making
after training), is the model deployed on edge devices?
[ Yes: NPUs or ASICs, for optimized power and
efficiency
O No: Consider FPGAs for flexibility or ASICs for
efficiency if the task won't change

® For models requiring future flexibility, are future
updates expected?
[0 Yes: FPGAs, due to their reprogrammability
[0 No: ASICs or GPUs, depending on whether the task is
more about speed or parallel processing
B Is low cost more important than cutting-edge
performance?
[ Yes: Consider older generation GPUs or cloudbased solutions where hardware costs can be easily
absorbed
m Will there be a need to quickly scale processing power?
[0 Yes: Cloud GPUs or TPUs can offer scalable resources
as required

A hospital is integrating a system that can automatically diagnose diseases from patientimaging data.
a

Describe whether this system should be classified as artificial intelligence, machine
learning or deep learning.

b

Distinguish between regression-based and classification-based machine learning.

An email client uses a program to sort incoming emails into “Primary”, “Social”,
“Promotions” and “Spam” folders.
a

Identify whether this is an example of supervised or unsupervised learning.

b

Describe your reasoning for this choice.

An autonomous vehicle company transfers the knowledge from a model trained in one city to
a new model designed to navigate another city.

a

Define “transfer learning”.

b

Outline how this is an example of transfer learning.

¢

Qutline one possible limitation to the effectiveness of this approach.

d

The original model was trained from thousands of hours of driving on roads under human

supervision to monitor and correct it when required. Describe the form of machine
learning used for the original model.
A tech start-up is planning to deploy a large-scale machine learning system to predict stock
prices in real time.
a

Identify one type of hardware that would be critical for processing large volumes of realtime data in this context.

b

Qutline one reason that this type of hardware is suitable for real-time data processing in
machine learning applications.

¢

Discuss one potential limitation of the identified hardware when used for
machine learning.

A university plans to implement an Al-driven system to analyse video lectures for enhancing
online learning experiences.
a

lIdentify two types of hardware that could be used for conducting machine learning
processing of video data in real time.

b

For the two types of hardware identified, outline one possible reason for selecting each
device over the other.

A4 Machine learning

Data preprocessing (HL)

SYLLABUS CONTENT

(;Tnp tip!
Spend significant time
on data preprocessing,
visualization and
analysis.
Understanding the
data is as important
as understanding the
algorithms.

I
o
Z
=<
EE

By the end of this chapter, you should be able to:
» A4.2.1 Describe the significance of data cleaning
> A4.2.2 Describe the role of feature selection
> A4.2.3 Describe the importance of dimensionality reduction

A4.2.1 Data cleaning
High-quality dara builds high-quality models. If the training data is full of errors or redundant

features, the model will learn from these inaccuracies and make poor predictions.
Taking the time to ensure your data is as clean as possible will reap rewards with respect to
efficiency and accuracy. There are several steps that may be useful for cleaning your data set.
# Outlier: a data point
that deviates from the
typical pattern of values

1

Handling outliers: Statistical methods, such as using the interquartile range or Z-scores,

can detect outlying data. Once found, depending on the context, outlying data may be
capped, transformed or removed as appropriate.

in a data set, indicating

a possible unusual or
erroneous value that
should be discounted.

r

1
1 Python
1
: import numpy as np
:

# Create random array of wvalues between 0 and 100

I

# Set one extreme value to act as an outlier

1

data = np.random.randint(0,

1

100,

size=1000)

| data[e9s] = 937
:

# Calculate outliers via Z-scores

| mea = np.mean(data)
:

std dev = np.std(data)

Ii

z = scores =

(data - mean)

/ stdLdev

1

threshold = 3 # Outliers if 3

:

outliers = data[np.abs(z_scored)

:

print ("mean",

mean,

"stddev",

stddev from mean
> threshold]

std_dev)

| print("Outliers:", outliers)
: # Calculate outliers via IQR
: gl = np.percentile(data,

25)

I g3 = np.percentile(data,
: igr = g3-gl

75)

:
1
1
1
1
1
1
1
1

cutoff = 1.5 * igr
lower bound = gl - cutoff
upper bound = g3 + cutoff
outliers = data[(data < lower bound)
print ("Outliers:",

\

(data > upper bound} ]

outliers)

L

Ad.2 Data preprocessing (HL)

@

Removing duplicate data: [dentifying and removing duplicate data will assist in preventing

ATNO TH

2

the model from becoming biased towards over-represented values. For data sets where
individual records contain a large number of variables, calculating and comparing SHA256

hash values can be a useful mechanism for detecting duplicates (see Section B4.1.6 for more
about hash values). Depending on the context of the model, near-duplicate data may also
need to be consolidated into a single record.

3 Identifying incorrect data: Process your data through validation rules to ensure obviously
incorrect data can be found and removed. This may mean checking the ranges given for
dates and times, or amounts given for currency values, and so on. Set sensible limits and

have your program detect anomalies for possible manual checking.
4 Tiltering irrelevant data: If there is no measurable correlation between an input variable
and the outcome variable, it may be completely irrelevant and contribute nothing to the

predictive power of the model. Keeping such data in the training process is only going to
make the process less efficient and less accurate.
Additionally, just because data may appear to be correlated doesn’t mean it is. As the
Spurious Correlations website demonstrates, if you compare enough unrelated data sets,
you will find correlations that are, in fact, not.
The number of movies Tom Hanks appeared in

Robberies in Alaska

cormelates with

correlates with

The number of special education teachers in Georgia

Professor salaries in the US

P
g

:
£

a

2.0

T7K

T2

T.0K

ss

2

-

83K

S—

b

57K

3e
B

T

I

i

T

1

1

T

1

1

T

1

2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022
nurr

#—The

T

r

=0l

L:
B

2

ks

af

he

ire:

ion

|

%

¥
8

&
3

@

B8

]

1287

$130.6K

11449

$137.3K

g

$135.0K &

i

T

T

2013

i

T

2015
ts in Ala:

M

T

2017

T

2019
FBI

S

2021

i

Cr

i
L

30

$132 6K

e

e

1

!

Jl

1

Center

(
W Tom Hanks movies vs special education teachers in Georgia

|

wdary

q

)
i

!

|

I
W

A
T

K

\spuri

jon

M Robberies in Alaska vs professor salaries

5 Transform improperly formatted data: Data may be incorrectly formatted but easily
correctable to ensure consistency in what is presented to the machine learning model,
for example:

O Ensure all dates are in a consistent style (not having a mix of day / month / year, month
/ day / year, or 1SO yyyy-mm-dd formats).
[0 Ensure numerical values are formatted, and to the same level of precision.

[ Ensure images are correctly rotated and oriented, and of matching ratio and size.
6

Missing data: Sometimes it may be necessary to use models to predict missing values to
ensure tull coverage of the data set. Mean / mode imputation, k-nearest neighbours or

7

Normalization and standardization: Many machine learning algorithms will benefit from
completing preprocessing of data by performing the statistical operations of normalization

regression models could be used for this, if required.

and standardization to scale data to a standard range or distribution.
00 Normalization can be used to rescale input data to a range of [0,1] or [-1.,1], which is
useful when various features (input variables) have different scales.

O Standardization can be used to transform the input data to have a mean score of 0 and
standard deviation of 1 (Gaussian distribution). (Note that it is not mathematically

possible for the range to be [-1,1] and to have a standard deviation of 1; you need to
determine which is required for your model.)
A4 Machine learning

(;Common
mistake

]

Python

1
1
1
1

import numpy as np

Ignoring the
important role that

: data = np.array([10, 20, 30, 40, 50])
1 # Normalize the data to have a mean of 0, and have range

normalization and

:

data_mean centered = data - np.mean(data)

standardization play
Recognize that
normalization (scaling
data to a range)
and standardization

:
1
:
:
1

max abs val = np.max(np.abs(data

:

standardized
data =

have zero mean

1

print (standardized data)

and unit variance)
are crucial for many
algorithms to perform
optimally. Appl

L

(scaling data to

=
o
Z
kg

=<
[-1,

1]

meancentered))

normalized data = data mean centered/max_abs
val
print (normalized data)
# Standardize the data to have a mean of 0,

and std dev of 1

(data - np.mean(data))/np.std(data)

.

tese ransinmations

A4.2.2 Feature selection

consistently across
all data used in
the model.

(
. TOK
How does the way that we organize or classify knowledge affect what we know?
The structuring of data sets and the choice of features directly influence the insights gained from
machine learning algorithms.
The way data is structured can significantly determine what the machine learning model can learn.
For instance, missing values; the inclusion or exclusion of certain data points; or the way categories
are defined and labelled can all skew or bias the model’s outputs. This structuring determines how
the machine “views" and “understands” the world, directly influencing the patterns it recognizes
and the predictions it makes.
The features chosen can amplify or suppress certain patterns within the data. For example, in a
model predicting creditworthiness, choosing features like income might reflect economic factors,
whereas including features like zip code could inadvertently introduce socio-economic biases related
to geographical areas.

# Feature: a numeric
property that can be
used to contribute a
data point for a machine
learning algorithm to

The decisions made in data structuring and feature selection are not value-neutral. They reflect the
biases, perspectives and priorities of those who design the data sets and algorithms.

Feature selection refers to taking care to select only the most relevant features for use in your
machine learning models. In the context of machine learning, a feature is a variable that

train on. Think of it as a

you wish to use as input values for generating predictions. While it may seem like a lot of

variable in your data set.

additional effort to perform manual fearure selection, the process can dramatically impact the
overall performance and accuracy of your machine learning model.

(;Com mon

Removal of irrelevant detail will result in a more generalized model that is better suited to

mistake

processing new, previously unseen data.

D

Three commonly used methods to help determine which features to select are filter methods,

the importance of

wrapper methods and embedded methods.

feature selection and

than the choice of

model itself.

. See also
sssssssnnns

engineering. Good
features are often
more important

For more detail on these approaches, along with example code, search online for scikit-learn’s
section 1.13 "Feature selection” documentation (https://scikit-learn.org/stable/modules/
feature_selection.html).

Ad.2 Data preprocessing (HL)

@

ATNO TH

M Filter methods
So-called as they help “filter out” features, filter methods involve applying a statistical metric

to determine which features are best to be retained and which should be removed from the
model. Features are ranked by their score, and those that don’t meet the threshold can be
filtered out.
As a purely statistical measure, using filter methods is less computationally expensive than
retaining the feature in the model for full training. The downside is this does not detect
interaction between features. That is, if one feature is affecting another, then a filter may
suggest deleting a feature that is actually important. This is where manual appreciation of the

context of your model is always important.
The most common, and easy-to-use, filter is to calculate the r value of the correlation
(Pearson’s product moment correlation coefficient). The r value of a data set may be

calculated using
. Ex, =¥, -¥)
where x and y, are your individual data points and % and § are the mean of each data series.
Omnce calculated, records with r values beyond a given threshold can be flagged for deletion.

B Wrapper methods
Wrapper methods involve iterating over different combinations of the input features and
comparing which subset produces optimal performance.
Selecting the best subset

/’/‘—\

set of all features ——»

generate a
subset

_ learning
——» performance
algorithm

w

B Wrapper methods

This can be a time-consuming and computationally expensive process, especially when
compared to filter methods. There is also an increased risk of overfirting the model. The
Research skills:
Select and analyse
an existing opensource data set

benefit, however, can be a very quick and efficient final model at the end of the process.
For further study on suitable techniques, do some research into recursive feature elimination
(RFE), and sequential feature selection (forward selection, backward elimination). The scikit-

learn library (online) provides functionality for both.

relevant to a

specific machine
learning problem.
Learn about the
data cleaning and
feature selection
process used by
these “professional”
projects, and make
recommendations
for students
learning to use
data-cleaning
methods for the
first time.

B Embedded methods
Embedded methods draw on both filter and wrapper methods, but incorporate them directly
into the model training algorithm. This means that the feature selection is performed

simultaneously with the model training, rather than as a separate step before training.
Embedded methods can be more computationally efficient since they don't require separate
iteration of the data prior to training. An embedded method will automatically assess the
relevance or importance of features and adjust their weights or inclusion in the model
accordingly during the training process.

While embedded methods can save manual labour by eliminating the need for feature selection
processes prior to training, they typically require more computational time compared to

simpler filter methods. The effectiveness of embedded methods depends on the model’s ability
to accurately assess feature relevance during the training process.

A4 Machine learning

A4.2.3 Dimensionality reduction
When gertting started with machine learning, it is easy to make the mistake of giving too much
data to your model. While more quality entries in your data set is usually good, supplying too
many features for each entry can easily cause more harm than good.

A typical way of thinking about this as a beginner would be “The more attributes or features
1 supply, the more detail about my data the model will learn, and perhaps it'll discover a
pattern thatT hadn't thought of”. The problem is that machine learning algorithms are at their
best when they are able to make generalizarions about the training darta. If there is too much
detail in each item, and not enough items overall to compensate for that extra detail, then
challenges arise.
# Curse of
dimensionality: each
feature in a machine
learning model adds
another dimension
to the overall model
the algorithm is

These challenges are known as the curse of dimensionality, and describe the problems
that arise in highly dimensional data. The following visualization is a useful way to help
understand the problem.
1d space

2d space

2

attempting to map and

create generalizations

—sbodecion

°

about; the curse of

dimensionality refers
to the problem that
occurs when there are
too many dimensions
relative to the quantity
of data available, so

that patterns cannot be
meaningfully observed.
# Data sparsity: how
“spread out” data
points are from each
other in a model.

3d space

|

—

@

hd ™

L

otol

[]

ool

o

S

NG|

>

B
0

Density: 10 + 4=2.5

Density: 10+ 16 =0.625

A
10

Density: 10 + 64 = 0.156

20

30

40

50

d

M Data increases in sparsity as more dimensions are added

In the first panel, there are 10 data points in one dimension, which represents one feature or
variable that the model is training with. With 10 points spread across a range of [0,4], there are

2.5 data points per unit. Visually, you can see it is quite crowded, meaning there is a lot of data
available to make conclusions and generalizations from.

In the second panel, the same 10 data points are now spread across two dimensions. While
both dimensions still have the range [0,4], the effect of the extra dimension is that it squares

the space available, so those 10 data points now spread out such that there are only 0.625 data
points per unit.
In the third panel, the third dimension is added. With three dimensions, representing three
features or variables, there is now only one data point per 0.156 units of space.
The additional detail that comes from adding the extra dimensions acts to spread the data out,

making it a lot more difficult for the model to find the generalizations it needs to be useful. To
keep the ratio of data points to space consistent, the third panel needs 160 items in its training

data instead of just 10. If you don’t compensate for additional dimensions with additional
quantity of data, the quality of your model will deteriorate.

The empty cells in the diagram above are an example of data sparsity, which is where the data
points are too far from each other, and the data set contains a high number of empty values. If

asked to generate a prediction when given those values that are empty in the training set, the
model will have no basis on which to make an accurate estimation.
Sparsity is problematic as it makes it difficult for models to find patterns without overfitting,
which is where the model effectively memorizes the individual items in the data set, including
the noisy little details.

Ad.2 Data preprocessing (HL)

ATNO TH

Distance metrics, such as calculating the Euclidean distance between points, lose meaning as
the distances between all pairs of points are similar (a long way away). Without being able to
find the patterns needed to make generalizations, the model will not be useful with unseen,
untrained data when you need it to be.
An over-abundance of dimensions also poses challenges for you and those on your developer

team with respect to data visualization. The mental capacity required to visualize highly
dimensional data is very difficult or impossible, and most humans struggle to wrap their mind

around more than three dimensions. This lack of intuition will make it difficult to analyse the
patterns and relationships within your data.

While increasing the sample size will help compensate for additional dimensions with respect
to model accuracy, it does introduce its own issues. Large sample sizes require increased
processing time and capacity for training, and increase memory usage requirements. Every
additional dimension already adds an extra order of magnitude to the memory required by

the model, so the increase in sample size required not to lose model accuracy only exacerbates
the memory and processing requirements. This all works to reduce the accessibility of your
model for limited hardware environments such as mobile or portable computing, home / office
computing and those without specialized infrastructure such as GPUs.
Tor these reasons, it can often be better to reduce rather than increase the number of
dimensions in your machine learning model. Keeping it simple will help the model learn the

generalizations it needs and reduce the demands on your limited processing hardware.

(;Common mistake
Misunderstanding the goals of dimensionality reduction
It is important to bear in mind that dimensionality reduction does not always lead to better model
performance. The primary goal of dimensionality reduction is to simplify the model by reducing
the number of variables, which can help in some cases but might also lead to loss of critical
information. There is a careful balance to be struck with the retention of relevant data aspects,

which can take a lot of practice to get right.

B Reducing dimensions of existing data sets
You can either make decisions about which dimensions to reduce manually, or make use of

statistical tools to assist in the process.
Two commonly used statistical techniques to help reduce the number of features are PC/

(principal component analysis) and LDA (linear discriminant analysis). PCA and LDA are
beyond the scope of your course, but you don’t have to know how they work to be able to make

use of them in your IA (if you wish or need to), as scikit has the functionality built in.
PCA is used for dimensionality reduction without considering your data set labels. It’s good [or
data compression, visualization and speeding up learning algorithms by reducing the number
of input variables.
LDA is supervised in the sense that it uses your training data labels. It reduces a data set to

a specified number of dimensions in a manner that best discriminates between the classes,
based on their statistical properties. For this reason, it is particularly used to prepare data for
classification tasks.

A4 Machine learning

Sklearn, also known

import numpy as np

as scikit-learn, is a

from sklearn.datasets import make classification

Python module for
machine learning built
on top of SciPy and
distributed under the
4-Claude BSD licence.

from sklearn.decomposition import PCA

To install this module,
please refer to the
instructions at https:/
scikit-learn.org/stable/
install.html

from sklearn.discriminant analysis import
LinearDiscriminantAnalysig as LDA
# Generate synthetic data

# x will be 2d array of 1000 rows,

20 columns

# v will be 1d array of integers of values 0,

1 or 2

%, y = make classification(n samples=1000, nfeatures=20,
n_informative=10, n_redundant=10,
nclusters
per class=1l, n classes=3)
print ("Original Data Shape:",
#

x.shape)

PCA

# Transform sample data from 1000x20 to 1000x2

pca = PCA(n_components=2)
%_reduced
pca = pca.fit transform(x)
print ("PCA Reduced Data Shape:",
#

x reduced pca.shape)

LDA

# Transform sample data from 1000x20 to 1000x2
lda = LDA(n_components=2)
¥_reduced
lda = lda.fit transform(x, y)
print ("LDA Reduced Data Shape:",

x reduced lda.shape)

@ See also
For more about using scikit-learn tools to help reduce dimensions in your data sets, search
online for scikit-learn’s section 1.2 “Linear and Quadratic Discriminant Analysis” (https://scikitlearn.org/stable/modules/Ida_qda.html).

(.-Top tip!
For discussing data preprocessing needs in an examination
setting, be sure to attain a thorough understanding of the
different role and effect of each one.
Some key points to remember for each one:
B Inputting missing data: The input of missing data will
improve model accuracy through providing a complete
data set, but it can introduce bias if the resulting data set
does not match actual data distribution.
B Deleting missing data: Simplifies the model by
removing incomplete cases to reduce overfitting, but this
could lead to the loss of valuable data.
B Removing duplicates: Enhances reliability and prevents
skewing of results.
B Removing outliers: The model can become more
generalized as it prevents extreme values from
disproportionally influencing predictions.

Ad.2 Data preprocessing (HL)

Filtering irrelevant features: By concentrating on
what is most relevant, the model will perform better and
faster. It reduces the risk of overfitting.
Normalization and standardization: IVost algorithms
perform better when features are all on a similar scale.
Filter methods: Computationally less expensive, they
can be used regardless of model type.
Wrapper methods: Provide better performance as
they consider feature interaction and are tailored to the
model in question. If the data set is small, it can lead
to overfitting.
Embedded methods: A balanced approach between
filter and wrapper methods, their effectiveness is
dependent on the model they are designed for.

ATNO TH

(;Top tip!

1

A marketing firm uses machine learning to analyse customer survey data to improve
targeting strategies.

a

Describe one common issue in survey data that would necessitate data cleaning.
Describe how feature selection could impact the performance of a machine learning
model in this scenario.

¢
2

3

Qutline the role of dimensionality reduction in handling high-dimensional data such as
survey responses.

Afinancial analytics firm uses machine learning to predict stock-market trends based on
historical data.

a

List one common data-quality issue that might require cleaning in this historical stock data.

b

Describe the role of feature selection in improving model performance in financial predictions.

¢

Describe the importance of dimensionality reduction on model complexity
and performance.

Aschool district analyses standardized test results to predict student performance and
identify at-risk students.
a
b

List one common data issue that might arise with standardized test-result data.
Describe the possible implications if the school district was to import raw test data for all

questions completed by students into the machine learning model.
¢

Qutline two commonly used methods of feature selection that could be beneficial in this
educational context.

A4 Machine learning

Machine learning approaches (HL)
SYLLABUS CONTENT
By the end of this chapter, you should be able to:
> A4.3.1 Explain how linear regression is used to predict continuous outcomes
> A4.3.2 Explain how classification techniques in supervised learning are used to predict
discrete categorical outcomes
» A4.3.3 Explain the role of hyperparameter tuning when evaluating supervised
learning algorithms
> A4.3.4 Describe how clustering techniques in unsupervised learning are used to group

data based on similarities in features
P> A4.3.5 Describe how association rule learning techniques are used to uncover relations
between different attributes in large data sets
P> A4.3.6 Describe how an agent learns to make decisions by interacting with its
environment in reinforcement learning
» A4.3.7 Describe the application of genetic algorithms in various real-world examples
> A4.3.8 Outline the structure and function of artificial neural networks (ANNs) and how

multi-layer networks are used to model complex patterns in data sets
» A4.3.9 Describe how convolutional neural networks (CNNs) are designed to adaptively
learn spatial hierarchies of features in images
» A4.3.10 Explain the importance of model selection and comparison in machine learning

(‘Key information
Before proceeding into this chapter, it is important to note that the syllabus content statements
above are limited to “Explain”, “Describe” and “Outline”.
This chapter intentionally contains additional detail that is beyond the syllabus, such as programming
code samples demonstrating the use of various algorithms. You do not need to be able to
read or write programming code for the different algorithms presented here in your
IB examinations.
The additional detail has been provided due to machine learning being an extremely popular
subset of Computer Science. In discussions with students, many expressed a desire to learn beyond
descriptive theory for these topics, and a wish to know how these algorithms work and how to use
them. Students also commonly express an intention to experiment with machine learning algorithms
within their internal assessments. For these reasons, this introduction is more in-depth than is
required solely for the examinations.

# Linear regression:
a machine learning
algorithm that seeks a
linear line of best fit for
a given data set, from
which extrapolations
can be made.

A4.3.1 Supervised learning: linear regression
Linear regression refers to calculating the correlation and line (or plane) of best fit among the
values of a data set, and then using the resulting equation of the line to make predictions for

new, unseen data.
Linear regression is one of the earliest machine learning algorithms to be developed, and can
even be calculated manually for limited data sets as they are purely mathematical constructs.

Ad .3 Machine learning approaches (HL)

I
o
Z
=<
EE

B Given a person’s height, is it possible to predict their weight?

Always check the
relationship between
variables with scatter
plots to see whether
they are roughly linear.
There is no point using
linear regression if it
doesn't fit the data.

B Given the number of hours a student studies for a test, is it possible to predict their result?
B Given the dollars spent on advertising a product, is it possible to predict the sales volume?
B Given the size of a home in square meters, is it possible to predict its sale price?

As hopefully you can infer from the name, linear regression is only suitable where there is a
linear relationship between the independent and dependent variables.
The graph below illustrates a simple form of linear regression with one independent variable
(the predictor) and one dependent variable (the response).
To explore the process involved for linear regression of one

200

independent and one dependent variable, the equation for
the line of best fit can be calculated using the least squares

175

o
N

dependent to v, you can use the standard equation of a line

2

By assigning the independent variable to x, and the

A

the line and individual data points.

to make predictions.
N

A

Dependent variable

regression line, which will minimize the distance between

y=a+bx

u

AINO TH

Linear regression can be used to help answer such questions as:

(; Top tip!

The slope, or gradient, of the line, b, represents the amount

that the prediction will change for every increment of one in

25

the independent variable. To calculate b, you find the sum of

20

o

a0

100

Independent variable
M Linear regression example

the difference between each point of the line from the mean:
b

_ X X0, -3

Ilx, — %0

The point of intercept, a, is the baseline value for the dependent variable when the independent

variable is 0. To calculate a, take the coefficient multiplied by the mean of x and subtract it
from the mean of y.
a=y-bx
As linear regression is such a common and popular task, both NumPy and scikit-learn libraries
have tools built in to perform these calculations for you. The following example uses scikitlearn, as we will use the library a lot for other algorithms coming up.

Python

1
1

import numpy as np

1
1

1

import matplotlib.pyplot as plt

1

1

from sklearn.linear
model import LinearRegression

1
1

# Sample data

x = np.array([45,48,65,68,68,10,84,22,37,88,71,89,89,13,59,66,40,88,47,89])
y = np.array([98,92,134,135,136,30,175,54,70,182,148,169,187,20,126,142,90,186,
99,176])

x = x.reshape (-1,

1)

model = LinearRegression()
model . fit(x,

v)

1
1

1
1
1

# Using scikit, calculate the model
# Convert the 1D array of 20 columns,

1
1

to a 2D array of 20 rows 1 column each

1
1

1

1
1
1
1
1
a

A4 Machine learning

intercept = model.intercept

o
Z

==

slope = model.coef [0]

# Using scikit,

generate a prediction where the independent variable is 70

X_test = np.array([[70]])

-

# New single data point for prediction

y_test predict = model.predict(x_test)
print (f"Prediction for independent variable value 70:

{y test

predict [0]}")
# Using matplotlib, plot the data and the line of best fit
xline = np.array([[0],

[100]])

yline = np.array([[intercept],

plt.scatter(x,

y,

[intercept+100*slope]])

color="blue")

plt.plot(x_line, y_line,

color="red",

linewidth=2)

plt.xlabel ("Independent variable")

plt.ylabel ("Dependent wvariable")
plt.title("Linear Regression example")
plt.show()

P
T P T
Y

ssssssssnnss

@ See also
Reference the scikit-learn documentation for supervised linear regression by searching online
for scikit-learn’s “LinearRegression” (https://scikit-learn.org/stable/modules/generated/sklearn.
linear_model.LinearRegression.html).
PA
T FT T T Y

Bl Measuring accuracy
# R-squared value
(or coefficient of

Assessing the accuracy of the model can be performed by calculating the R-squared value, also
known as the coefficient of determination. It is a measure of the proportion of variation in

determination): a

values from the independent variable data, and what the model would have predicted for that

statistical measure that

point. A value close to 0 indicates the model does a poor job of explaining the relationship of

indicates how well

the linear regression

model fits the data
points given.

the data, whereas a value close to 1 indicates the model does an excellent job of mapping the
relationship of the dara.

Why use a measure that relies on squaring? Why not just take the average of the absolute value
of the difference between the predicted and actual values? There are a few reasons:

B Squaring means larger errors will become more pronounced than smaller errors. Absolute
differences would treat all variations the same.
B Squaring results in a function that can be differentiated; a very handy benefit for

optimization algorithms that rely on derivatives. Absolute values in a function cannot
be differentiated.
m Squared differences are used within classical statistics with respect to assumptions around
normalization, so it is convenient to stick with that approach.
There are approaches, other than R-squared value, that can be used, including:
® adjusted R-squared: modifies the formulae to account for the number of predictors in

the model
mean squared error: the average of the squares of the errors

mean absolute error: the average of the absolute value of the errors
B mean absolute percentage error: the average of the absolute percentage errors of predictions.
Ad .3 Machine learning approaches (HL)

@

ATNO TH

With NumPy, the r-squared value needs to be calculated through each step, whereas scikitlearn has a built-in merhod for the rask.

Python
# Method 1: Using numpy
y predicted = model.predict
(x)
y_residuals = y - y predicted # Difference between actual and predicted values
ss res = np.sum(y residuals**2)

# Sum of squares of the residuals

ss_tot = np.sum((y - np.mean(y))**2)

rsquared = 1 -

# Total sum of squares

(ss res / ss tot) # R-sqguared value

print (f"R-squared: {rsquared:.z2f}")
# Method 2: Using scikit-learn
r_squared = model.score(x,y)

print (£"R-squared: {rsquared:.2f}")

Bl Multidimensionality
The previous example illustrates simple linear regression where there is only one independent
variable. In real-world applications, it is highly likely that you will want to model against

several independent variables. This is known as “multiple linear regression”.
While most humans can mentally visualize data in two, or perhaps three, dimensions, it

becomes extremely challenging to visualize beyond that. The good news is our software tools
have no problem modelling the relationship across multiple dimensions. Here is an example

using scikit for a linear regression model that has four independent variables.

Python
import pandas as pd
from sklearn.model selection import train test_split
from sklearn.linear
model import LinearRegression
# Load the data and assign features

# (dependent variable)

(independent variables)

to x,

and the target

to y.

df = pd.read csv("Multidimensional example.csv")

1
1
1
1
1
1
1
1
1
1
1
1
:

x = df[["Feature
1", "Feature
2", "Feature
3", "Feature 4"]]

:

y = df["Target"]

1

# Train the linear regression model

1
1

model = LinearRegression()

:

model.fit (x, y)

1

# Generate prediction for new data point

:

new point = np.array([[50,

-150,

30,

1001])

:

prediction = model.predict (new_point)

print (£"Predicted target for the new sample:

:

{prediction[0]}")

1
1
Ll

The Multidimensional_example.csv file can be downloaded from https:/github.com/
paulbaumgarten/hodder-ibdp-computerscience

The prediction for the new data point used in the code should be 510.7.

A4 Machine learning

GTop tip!
Regression with non-linear data?
If you have a data set you would like to fit to a non-linear function, the scipy curve
function is what you are looking for.

fit()

For example, assuming you have NumPy arrays of x _ data and y _ data that you wish to fitto a
quadratic function:
_________________________________________________ _

Python
import numpy as np
from scipy.optimize import curve fit
xdata =

[....... your data here....... 1

ydata =

[....... your data here....... ]

def myquadratic(x,

a,

b,

return a * x**2 + b *

params,
x_data,
a,

b,

c):
x + ¢

params covariance = curve fit(my quadratic,
y_data)

¢ = params # Extract the fitted coefficients

For more information, search online for scipy.optimize.curve_fit (https://docs.scipy.org/doc/scipy/
reference/generated/scipy.optimize.curve_fit.html).

A4.3.2 Supervised learning:
classification techniques
# Classification
techniques: where
a machine learning
madel has been
trained to identify,
from a predefined list

One task frequently required of machine learning algorithms is to classify data as belonging

of categories, which

making processes and can be applied to a wide range of practical problems. Examples of

category (or class) the
input data would most
likely be associated with.

everyday classification problems include:

to one of a given range of categories. Whereas the output prediction from linear regression
is numeric, classification algorithms generally produce a non-numeric value to represent
a category.
Categorization through machine learning is useful as it facilitates automation of decision-

email spam detection (spam or not spam)
m medical diagnosis (disease or no disease)
B credit score (good credit risk or poor credit risk)
B image recognition (identifying what category of object is in the image)

m natural language processing / sentiment analysis (positive or negative sentiment)
B recommendation engine (what genre of movie to suggest next).
Two popular methods that do not require neural networks are k-nearest neighbours and
decision trees.

Ad .3 Machine learning approaches (HL)

ATNO TH

B K-nearest neighbours
# K-nearest
neighbours: where
data points are
categorized based on
the categories of the
nearest points around
them in the data set; k

is a variable representing
how many of those
nearest points should
be used to "vote”
and determine what
category to assign the
new value.

K-nearest neighbours is a machine learning technique that allows classification of data based
on patterns learned from existing labelled data.

(;Top tips!
u Normalize or standardize data because KNN is sensitive to the magnitude of data points.
B Choose an odd number for k when the number of classes is even to avoid tie situations.
B Experiment with different distance metrics (e.g. Euclidean, Manhattan) to see which performs
best for your data set.

-

3]
*
.

2

.

~N
g

.

=

3

A

[ ]

=

4
:

i

H 1

0

[

.

e
.

0

.

.

"

1
.,

T

-

.

ol

¢

.
e

1

-

-

T

T

T

T

T

-2

-1

0
Feature
1

1

2

3

B K-nearest neighbours
Consider the chart above, in which the data have been classified into either blue dots or red

dots. The two axes represent the different features (variables) that have been measured. These
can represent anything (such as size, weight, length, time, cost, review rating), provided they
can be measured and plotted on a numeric scale.
The green dot represents a new value that the model has not seen before. How can k-nearest
neighbours be used to determine whether the green dot should be classed as belonging to the
blue group or the red group?
With KNN, a value k is selected 1o represent how many nearby data points should be used to
determine the prediction output. The algorithm will then determine the k-nearest points, and
allow each of them to “vote” as to which final category the prediction should award.
The following charts illustrate the decision boundary for different values of k.
n_neighbours = 1

n_neighbours = 3

4

4

3

2

~ 2

3

.

[
o

£ 1

~ 2

. .' .

.

22
.

[:1]
g

>

21

2

&

0

-

0

-1+

L]

-1

I

[

-2

2

1

|

3

3

Feature 1

B KNN where k=1

B KNN where k=3

A4 Machine learning

n_neighbours = 5

In the examples here, values of k of 1, 3 and 5 have

4
3

M

; 2

214
& o

=

been used.

,:E

When k = 1, the line represents the boundary to the nearest

g

single point of either category. Normally, this is susceptible

>3

8

to being influenced by outliers, so avalueof

B

is more typical.

k=3 or k=5

L

It is important when selecting between two categories to

.

ensure that k is an odd number to avoid the situation where

N

there could be a tie!
2

2

3

The data set for the charts shown here is the knn_dataset.

Feature 1

csv file that can be downloaded from https://github.com/

B KNN where k=5

paulbaumgarten/hodder-ibdp-computerscience

Python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsClassifier
# Load the data

N3

X

df = pd.read csv("knn dataset.csv")
= df [["Feature
1", "Feature 2"]].values

= df ["Label"] .values
New,

unknown point to classify

= np.array([[0.4,

1.6]])

# Create and fit the KNN classifier with 3 neighbours
knn = KNeighborsClassifier
(n neighbors=3)
knn.fit(x,

v)

# Plot the training points
plt.scatter(x[:,

0],

x[:,

1],

c=y)

plt.scatter(z[:,

0],

=z[:,

1],

color="red",

zorder=5)

plt.xlabel ("Feature 1")
plt.ylabel ("Feature 2")

plt.title("n neighbors=3")
plt.xlim(xx.min(),

xx.max()})

ple.ylim(yy.min(),

vyy.max())

plt.show
()
# Make the prediction

predicted category = knn.predict(z)
print ("The predicted category for point z is:",
predicted category[0])

One example application of KNN is in the development of collaborative filtering
recommendation systems. Consider a scatter plot for two movies that a wide number of users
have reviewed and rated. The x axis may represent the ratings by each user for movie A, and

the y axis may represent the ratings by each user for movie B.

Ad .3 Machine learning approaches (HL)

@

ATNO TH

Mark a new point on the plot for the target user, who has viewed movie A but not movie B.
By referring to the other points on the scatter, KNN can infer what the target user would rate
movie B. If the prediction is tor a high rating, the model can recommend the user to watch

that movie.
(In reality, rather than performing KNN on pairs of movies at a time, a multidimensional

approach would be taken, comparing ratings of many movies at once.)

ssssssssnnns

See also
Reference the scikit-learn documentation for supervised k-nearest neighbours classification by
searching online for scikit-leamn’s section 1.6.2 “Nearest Neighbors Classification” (https://scikitlearn.org/stable/modules/neighbors.html#nearest-neighbors-classification).

M Decision trees
While KNN is considered a lazy learner (it does very little during the training phase and defers
# Decision tree: a

graphical representation
of conditions that will
result in a classification

decision being made;
think of it as a decisionmaking flowchart that
the machine learning
model creates.

most of the computation until prediction), decision trees are considered eager learners that

build a classification model during training.
Conceptually, you can think of decision trees as a large flowchart or series of nested ifelse statements that are used to determine classification. Rather than having to manually
determine the decision points and write programming code for the if-else statements yourself,
the algorithm will analyse the training data to automatically determine the cutoft values for
each decision point along the way, and how deep to make the nested tree.
Decision trees make for an easy-to-maintain algorithm since the model can be retrained and
its decision paths and threshold values subsequently adjusted based on new data. Decisiontree algorithms are a scalable solution that works with large and complex data sets compared
to the impracticalities associated with maintaining if-else statements yourself that might have
hundreds or thousands of decision points and pathways, and therefore also be very much

prone to human error.
Iris flower data set

Setosa

Versicolor

Virginica

M Iris flowers

The iris data set is commonly used as an introduction to decision trees. It contains
measurements of 150 irises (a type of flower), one-third each of setosa, versicolor and
virginica. For each of the 150 measurements, there are four features (variables):

B sepal length in cm
B sepal width in cm
® petal length in cm
u

petal width in em.

A4 Machine learning

One version of the final trained decision tree might look like this:

z
o
=

I
petal length (cm) <= 2.45

gini = 0.667

-

samples = 120

value = [40, 41, 39]
class = versicolor

/

N

gini= 0.0

petal length (cm) <= 4.75

samples = 40

sa?:;l;n_jso

"acll';'::_[:gt'oi'am

value = [0, 41, 39]

=

class = versicolor

/

TR

petal width (cm) <= 1.65
gini = 0.053
samples = 37
value = [0, 36, 1]
class = versicolor

petal width (cm) <= 1.75
gini = 0.206
samples
= 43
value = [0, 5, 38]
class = virginica

VAERN

gini = 0.0
samples = 36
value = [0, 36, 0]
class = versicolor

ey

petal length (cm) <= 4.95

gini = 0.0
samples = 1
value = [0, 0, 1]
class = virginica

/

petal length (cm) <= 4.85

gini=0.5

gini = 0.056

samples = 8
value = [0, 4, 4]

samples = 35

value = [0, 1, 34]
class = virginica

class = versicolor

gini =0.0
samples = 2

L\

N

petal width (cm) <= 1.55

sepal width (cm) <= 3.1

gini = 0.444
samples = 6
value = [0, 2, 4]
class = virginica

gini = 0.444
samples = 3
value = [0, 1, 2]
class
= virginica

value = [0, 2, 0]

class = versicolor

/
gini = 0.0
samples = 3
value = [0, 0, 3]
class
= virginica

e

N

petal length (cm) <= 5.45
gini = 0.444
samples = 3
value = [0, 2, 1]
class = versicolor

gini = 0.0
samples = 2
value = [0, 0, 2]
class = virginica

gini = 0.0
samples = 32
value = [0, 0, 32]
class = virginica

N\
gini = 0.0
samples = 1
value = [0, 1, 0]
class = versicolor

VAN

gini =0.0
samples = 2
value = [0, 2, 0]
class = versicolor

gini = 0.0
samples = 1
value = [0, 0, 1]
class = virginica

M Trained decision tree for the iris data set

Starting at the top of the tree, observe the first decision is whether the petal length is <= 2.45 cm.
The other values printed in the node advise about the model’s prediction if there was no

further processing beyond this point. The significance of the terms used are:
B

Gini indicates the decision tree would be 67 per cent uncertain in the prediction generated

(which makes sense as, without taking any branch in the tree, it will effectively be making
a 1-in-3 guess).
B Samples indicates that all 120 samples passed through this node (note it is 120 instead of

150 as, when this diagram was produced, 30 samples were retained as unseen for validation
testing purposes).

Ad .3 Machine learning approaches (HL)

ATNO TH

® Value indicates the spread of the three classifications at this point (40 setosa, 41 versicolor
and 39 virginica).
m Class indicates that the prediction at this point would be versicolor (since it had the most
samples, with 41).
Based on the measurement of petal length, you either take the branch to the left, if the petal
length is <= 2.45 cm, or to the right, if the petal length is > 2.45 cm.
Keep traversing the tree until you reach a termination point, or you have gone as deep into
the tree as you would like and wish to terminate, obtaining the model's best prediction at
that point. (Keep in mind that, while the model represented here only has a maximum depth
of six levels, more complex decision trees can easily have maximum depths of hundreds or
thousands of layers, hence the option for stopping once a particular depth is reached.)
The following Python will implement the iris problem.

.datasets import load_iris
test split
sklearn .model selection import train
sklearn .tree import DecisionTreeClassifier
sklearn import tree

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
# Load the Iris data set

iris df = pd.read csvi('"iris.csv")
x = iris df.drop("species", axis=1) # features are all the columns except species
y = iris df["species"]

# target variable

# Split the data set into a training set and a test set
X train, x test, ytrain, ytest = train
test split(x, y,

test size=0.2,

random state=42)
# Create a decision tree and generate predictive data
clf = DecisionTreeClassifier (max depth=10,

random state=42)

clf.fit(x train, ytrain)
y_pred = clf.predict(x_test)
# Plot the decision tree
plt.figure(figsize=(12,

tree.plot tree(clf,

8))

filled=True,

feature names=iris.feature names,

class names=

iris.target names)
plt.show()

# Validate accuracy using the test data
accuracy = clf.score(x test, ytest)
print (f"Accuracy of the decision tree classifier is:

{accuracy:.2f}")

# Generate a prediction for a manual data point
new_flower measurements = np.array([[5.0,

3.5,

1.5,

0.2]])

predicted species = clf.predict (new
flower measurements)
print (f"The predicted species for the new flower is:

{predicted_species[o] '

A4 Machine learning

The iris data set csv [lile can be downloaded from https://github.com/paulbaumgarten/hodder-

@ See also
Reference the scikitlearn documentation
for decision trees by

searching online for
scikit-learn’s section
1.10 "Decision Trees”
(https://scikit-learn.

org/stable/modules/

:

One real-world application of decision trees is to assist with patient diagnosis. The features

:

(variables) at issue in such a model may include demographic information (age, sex, height,

3

weight); clinical measurements (blood pressure, glucose levels, haemoglobin, cholesterol);

lifestyle factors (such as smoking status and alcohol consumption); along with symptoms,
conditions, medical history and test results. It is easy to envisage such a model having dozens

::

of features.

tree.html).

Thinking skills: Classification with KNN or decision trees?
Select a classification problem and perform a comparison analysis of both KNN
and decision trees to solve the problem. Evaluate the trade-offs between the two
approaches and create a decision matrix based on criteria such as ease of understanding,
computational efficiency, performance on small vs large data sets, and so on.

(;Top tip!
When choosing between k-nearest neighbours (KNN), decision trees and artificial neural networks
for a supervised learning classification scenario, consider the following:
M KNN suits moderate data sizes and low dimensions; decision trees handle mixed data sizes

well; neural networks excel with large, complex data sets.
B KNN requires normalization; decision trees need minimal preprocessing; neural networks
often require extensive preprocessing.
B KNN and decision trees are highly interpretable; neural networks are less so, and are often
considered "black boxes”.
m KNN are slow at prediction; decision trees offer fast predictions but can overfit easily; neural
networks require significant computational power but handle non-linear data well.
B KNN easily integrate new data; decision trees and neural networks most often require retraining.

A4.3.3 Supervised learning: evaluation and tuning
B Evaluation metrics

(®TokK
How can we know that current knowledge is an improvement upon past knowledge?
Machine learning requires evaluating the performance of new algorithms against benchmarks or
previous models.
In the context of machine learning, improvement is often quantified in terms of performance
on specific tasks. However, Theory of Knowledge invites you to question deeper aspects of this
improvement: Does performing better on a task, like object recognition, necessarily mean the
algorithm has gained more “knowledge”? Is the understanding deeper or merely more functional?
Additionally, metrics can sometimes be misleading. For example, an algorithm might score very
highly on accuracy but fail in particular scenarios that weren’t well represented in the training data.
Within supervised learning, there are several important, established metrics that can be used
to evaluate the effectiveness of your model.

Ad .3 Machine learning approaches (HL)

=
o
Z
kg

ibdp-computerscience

@

=<

ATNO TH

# Confusion matrix: a
simple pictorial means of
representing how well a
machine learning model
is performing.

The starting point would typically be to produce a confusion matrix. For a binary classification
problem, the data forms a two-row, two-column table modelled as follows:
Predicted

Actual

Predicted positive

Predicted negative

Actually positive

True positive

False negative

Actually negative

False positive

True negative

The number of scores that are true positive, false negative, false positive or true negative are

written into the respective cell (and typically colour coded with dark shading to indicate
higher quantities), the idea being that it is a quick visual indicator of the success of your

model. If the highest numbers (and dark shading) run down the diagonal of true positive and
true negative, then that is a good sign.
A confusion matrix can also be produced for higher dimensional classification problems. In
that instance, each possible classification would be turned into rows and columns. In this case,

the diagonal set of cells from top left to bottom right would again represent correct predictions.
Using your confusion matrix, you can proceed to calculate the accuracy, precision, recall and
F1 scores.
B Accuracy: The fraction or ratio of correct predictions

accuracy = correct pret_iic‘tions
total predictions
B Precision: The fraction or ratio of correct positive predictions to total positive predictions
For instance, of all images recognized as being “cars”, how many of them were correctly
classified? Alternatively, of all the “spam” predictions, how many of those were correctly
“spam”? This is important when a false positive may have a significant consequence.
precision

true positives

true positives + false positives
B Recall: The fraction or ratio of correct positive predictions to actual positives
For instance, this could be number of patients correctly predicted to have diabetes, out of
all the patients who truly have diabetes.
recall =

true positives
—
.
true positivies + false negatives

B F1 score: The harmonic-mean of precision and recall; it is particularly useful where both
false positives and false negatives may carry significant consequence
A harmonic-mean is different from an arithmetic mean in that it will always give a score
closer to the smaller of the two numbers. The F1 score will range from 0 to 1, with 1 being

the best score.
Fl score =2 » precision = recall
precision + recall
Consider using an F1 score in the criminal justice system, where an algorithm has been
devised to predict whether an individual will re-offend if released on parole. (Note that there
have been real-world problems in using machine learning modes for this exact scenario; refer
to A4.4 Erhical considerations for more.) In this situation, precision measures the correctness

of positive predictions. High precision means most individuals predicted to re-offend actually
did re-offend. Recall measures how well the model successfully identifies those who will reoffend, so that re-offenders are not being ignored by the system. Both are important for matters
of public safety, so combining them through the use of the F1 score is valuable.

A4 Machine learning

Another example scenario is to imagine a school using face recognition to automatically

record attendance as students walk through the school gate. In this scenario, high precision
implies that when the system identifies a student, it is identifying the correct student (rather
than recording the wrong student as present), and high recall suggests the system correctly

identifies most or all students who pass through the gate.

(‘Common mistake
It is common for students to over-rely on accuracy and neglect the nuance provided by the other
metrics. Have a clear understanding of the distinct roles of precision and recall.
B Accuracy is the overall correctness of the model (both true positives and true negatives).
B Precision is the proportion of positive identifications that were actually correct (important
when the cost of a false positive is high).
B Recall is the proportion of actual positives correctly identified (important when the cost of a
false negative is high).
B F1is the harmonicmean of precision and recall (useful when a balance between precision and
recall is needed).

#® Hyperparameter:
a parameter (or value

[ | Hyperparameter tuning
Hyperparameters is the technical term for the global variables thart attect the entire model.
Commonly used hyperparameters include:

assigned to a variable)

learning rate (neural networks)

that is set before the
learning process, which
quides the algorithm as
it learns.

activation function (neural networks)
number of hidden layers (neural networks)
maximum depth of tree (decision trees)
number of neighbours (k-nearest neighbours)
number of clusters (unsupervised clustering)
other variables as required by the model.
Hyperparameter tuning is the process of experimentation and adjustment of the combination
of parameters that results in optimal performance of a model.

(.-Key information
Hyperparameters exist in all types of machine learning, not just in supervised learning. Take the time
to identify the hyperparameters in whatever algorithm you are using and the effect their adjustment
will have.

B Overfitting and underfitting
(;Common
mistake
Avoid creating models
that are too complex
for your data; simpler
models are easier
to understand and
debug, and often
perform better on
new, unseen data.

Overfitting occurs when the model effectively memorizes detail from the training data that is
too fine grained for it to make sufficient generalizations for use on unseen data. Reducing the
depth of a decision tree, increasing the regularization strength in a linear model or reducing

the number of neurons in hidden layers may help with this problem.
Underfitting occurs when the model is too simple and hasn't learned enough detail about the
underlying patterns involved, such that the model also performs poorly on unseen data.
Signs of overfitting include:
B

the model performs significantly better on the training data compared to the validation data

B a complex architecture is used, with many features

Ad .3 Machine learning approaches (HL)

e
o
=

=

=<

ATNO TH

® the training error rate decreases, but the testing error rate increases after a given number
of epochs
B reducing the model's complexity improves test performance.
Signs of underfitting include:
B the model performs poorly on both the training and test data sets
B asimple model is used, with minimal features
B there are insufficient fearures to adequately capture the characteristics of the data
u

increasing complexity or adding features improves test performance.

A4.3.4 Unsupervised learning:
clustering techniques
To review, unsupervised learning is where the data set your model is trained on is unlabelled.
That is, you don’t supply the correct answer that corresponds with each datum you supply.

Rather than looking for data that is similar to known answer values, the features of the
unlabelled data are compared for similarities among them all, with the goal of identifying

naturally forming clusters in the groupings of data.

B K-means clustering
4 Clustering
techniques: where
data is grouped into
clusters based on
similarity or proximity to
each other without any
labels provided to help
indicate the correctness
of associating any
individual datapoint to
the cluster assigned.

K-nearest neighbours is very commonly used for unsupervised clustering, in addition to the

supervised learning approaches already considered.
With the supervised approach, when a new, unlabelled data point is introduced, the algorithm
measures the distance (often Fuclidean) from the new point to all those in the training set.
From there, it identifies the nearest neighbours to assign a predictive label to the new value.

With the unsupervised approach, when a new data point is introduced, the algorithm similarly
measures distances to other data points to determine its nearest neighbours but, instead of

predicting a label, it uses these relationships to identify the groupings, or clusters, within the
entire data set.
The main weakness with a KNN approach is that it assumes clusters are spherical and of
similar size, making it sensitive to inirial centroids and outliers.
Grade boundaries using k-means clustering
£

1

]

i | === Scores distribution

g8

&6

—

o4
E

E27

ilKey

N

1

[1

i

Il

z2,

1
0

20

40

60

80

100

M K-means clustering with one dimension

In this example, k-means clustering has been used to determine grade boundaries for a cohort
of 200 students. Asked to cluster the grades into six buckets, one for each letter grade, the

algorithm determined the following boundaries:
B A:8348to 100

m D:56.30 to 6445

B B: 7274108348

®m [:47.09 to 56.30

B

m T:0to47.09

C6445t072.74

A4 Machine learning

Moving from one to two dimensions makes the identification of clusters more accurate. In this
instance, k-means could look like the following:

=

o
Cluster of students based on study habits and test performance

'2

100}

=
Key

90|/ @Cluster1

.

® Cluster 2
@ Cluster 3

g
> 80

:
®

g 70 o

‘g 0

'~

|

&

LI - ...
PP
[]
.&
..

!.":’0 “9..
°®
@

°
°®

°
.‘

P

0

eIy
U
®__"oo it [ TP
oo

.: %0

2

5

P
v

[]

° ®

o

sl
on

.,

Ch

50

L

..

®

!

a

®

e

o0

40|

°*

o
T

0.0

2.5

T

T

5.0

T

T

T

7.5
10.0
12.5
15.0
Hours spent on homework per week

T

.

17.5

20.0

B K-means clustering with two dimensions

Health and lifestyle clusters

While providing a 3D visualization in the 2D medium of a

Key

textbook is problematic, hopefully the point still gets across

® Moderate

with the following illustration that, as dimensions go up, so

® Active & healthy

the identification of clusters becomes easier, given the data

® Low activity & poor sleep

A

.

.

.

points become further spread out. This works provided

l_g

there is still enough data to form actual clusters, otherwise
£
[=

|g €

.

E]

.

.

7

the curse of dimensionality will soon kick in (as can be seen
inthe illustration; there are many cells with no value).

=
@
a

-5

4
.
Physic
al actis
vity

6

P

(hoursfweek)

10

"

W K-means clustering with three dimensions
r

1
1
1
1
1
1
1
1
1
1
I
1

Python
# Example implementation of k-means clustering
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
# Generate synthetic data

L3

Ad .3 Machine learning approaches (HL)

1
1
1
1
1
1
:
1
1
1
1
-

@

# Students who spend little time but perform variably
groupl = np.random.normal (loc=[5,

60],

scale=[2,

10],

size=(50,

2))

# Students who spend a moderate amcunt of time and perform moderately
group2 = np.random.normal (loc=[10,

75],

scale=[2,

5],

size=(50,

2))

size=(50,

2))

# Students who spend a lot of time and perform well
group3 = np.random.normal (loc=[15,

90],

scale=[2,

5],

# Combine the groups into a single data set

data = np.vstack([groupl, group2, group3])
# Apply k-means clustering
kmeans = KMeans(n_clusters=3, random state=42)
kmeans.fit (data)

labels = kmeans.labels_
# Plot results
plt.figure(figsize=(10,
colors =

["red",

6))

"green",

"blue"]

for i in range(3):
plt.scatter(datal[labels == i,

0],

datallabels == i,

1],

color=colorsl[i],

label=f"Cluster {i+1}")
plt.title("Cluster of Students Based on Study Habits and Test Performance")

plt.xlabel ("Hours Spent on Homework per Week")
plt.ylabel ("Test Performance

(%)")

plt.legend
()

plt.grid(True)
plt.show()

(‘Common mistake
Assuming clusters are globular; k-means does not work well with non-spherical clusters.

® See also
sssssssnn

AINO TH

np.random.seed
(42)

100

Reference the scikit-learn documentation for k-means clustering by searching online for scikitlearn’s section 2.3.2 "K-means" (https://scikit-learn.org/stable/modules/clustering.html#k-means).
D
LT T E R Ty

B Spectral clustering

0.75-

Spectral clustering is another technique that is useful

= 0.50-

where clusters are not linearly separable. To classify any

,E 0.25

new data point, it will look at where the new point fits best

£ 0.00

among the groups already made, like finding which circle

-0.25-

of friends a new student would fit into at school.

—0.50—

M Spectral clustering

A4 Machine learning

Python

=

import numpy as np

o

import matplotlib.pyplot as plt

-~

2y
=

from sklearn.datasets import make moons
from sklearn.neighbors import kneighbors_graph
from sklearn.cluster import SpectralClustering
# Generate synthetic data

x,

(two interleaving half circles)

= make moons(n samples=300,

noise=0.07,

random state=42)

# Create a k-nearest neighbours graph
knn graph = kneighbors graph(x,

nneighbors=10,

include self=False,

mode="distance")

# Apply spectral clustering using the KNN graph
spectral = SpectralClustering(n clusters=2,
assign labels="kmeans", random_gtate:42)

affinity="precomputed",

labels = spectral.fit
predict (knn_graph)
# Plot the results
plt.figure(figsize=(8,
plt.scatter(x[:,

0],

4))

x[:,

1],

c=labels,

cmap=plt.cm.rainbow,

edgecoclor="k",

s=50)

plt.title("Spectral Clustering results")
plt.xlabel ("Feature 0")
plt.ylabel ("Feature 1")
plt.show
()

Social network analysis using spectral clustering

Spectral clustering may be useful in contexts such as social
network analysis to identify communities within networks

@ See also
Reference the scikit-learn documentation for spectral
clustering by searching online for scikit-learn’s section
2.3.5 “Spectral clustering” (https://scikit-learn.org/stable/
modules/clustering.html#spectral-clustering).

ssssssssssssasanns

by treating nodes as people and edges as their relationships.

B Hierarchical clustering
This approach builds a tree of clusters and doesn't require
the number of clusters to be specified in advance. It
provides a dendrogram (a tree-like diagram) to interpret
the data by viewing at different levels of granularity;
W Spectral clustering can group people according to social
networks

Ad .3 Machine learning approaches (HL)

however, it is computationally intensive for large data sets.

@

Example applications include genealogy research to analyse

8-

genetics to understand family relationships, and organizing

7+

library resources such as books and journals in a manner

° 61

that reflects similarity of content based on topics, themes or

g

authors.

£5

B4

.

.

T

a5,

In this plot of a family tree, individuals from the same

2]

family are grouped closer together first and, as you move

1-

up the dendrogram, families start merging based on their

0-

similarities (distances).
12

1013
11 14 7

8

6

5

9

4

1

2

0

3

Individual index
M Hierarchical clustering tree

@ See also

:

} Reference the scikit-learn documentation for hierarchical clustering with a dendrogram by
: searching online for scikit-learn’s “Plot Hierarchical Clustering Dendrogram” (https://scikit-learn.

-

: org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html).

H

l DBSCAN clustering
(.-Key information
DBSCAN stands for Density-Based Spatial Clustering of Applications with Noise.

DBSCAN: Identifying fraudulent transactions
me of day (standardized)

ATNO TH

Hierarchical clustering dendrogram

=

DBSCAN clustering will group together points that are
close to each other based on a distance measurement and
a minimum number of points. It is very effective for data
with clusters of similar density. Unlike k-means, DBSCAN
does not require the number of clusters to be specified. It
can find arbitrarily shaped clusters and can handle noise
and outliers.

4

s

T
-2.5

0.0

T

1

T

®

T

2.5
5.0
7.5
10.0
125
Transaction amount (standardized)

.

An example application might be to detect frandulent

T

150

M Density-based spatial clustering

financial transactions by clustering on similarities of
amount, location and time. DBSCAN can identify the dense

clusters that are “typical” and then separate out unusual
transactions for alerts.

R.
.
.
.
.
.
.
Reference the scikit-learn documentation for DBSCAN by searching online for scikit-learn’s
.
.
.
section 2.3.7 "DBSCAN" (https://scikit-learn.org/stable/modules/clustering.html#dbscan).
IOy .

@® See also

# Association rule:
a process of finding
patterns of co-

occurrence in data;

A4.3.5 Unsupervised learning: association rule

this means, given the
presence of one item in

The association rule can be understood as a data mining technique that seeks to find co-

a record, how likely it is

occurrences within a data set. It is another form of unsupervised learning. The technique

that another item will
be present.

behaviour tracking, and more.

is commonly applied for market analysis, as well as crime analysis, healthcare, web / app

A4 Machine learning

There are three key metrics associated with association rule learning:

=
o
=

e

B Support: The proportion of transactions that include a particular item or combination

of items.
m

Confidence: The likelihood of occurrence of a particular item (B) when given some other
item (A).

m Lift: The degree to which two items will appear together in this model, compared to the
expected likelihood of them appearing together if the items were statistically independent;
a lift value greater than 1 indicates the presence of item A increases the likelihood of
B appearing.
The following example uses a data set of transactions from a fresh food market to find the
items that are frequently purchased together. You can download the data set from

https://github.com/paulbaumgarten/hodder-ibdp-computerscience
The first table, frequent itemsets, shows the support value for each set of items. In this case,
84 per cent of transactions include the sale of Milk. The association rules table illustrates the
association between the antecedent (prerequisite) item and the consequent (resulting) item. In
this case, the table shows:
B 62 per cent of rransactions involve both Milk and Bread
m there is a 73 per cent likelihood that the customer will also purchase Bread if they
purchase Milk
B the lift of 1.05 indicates that Bread is 1.05 times more likely to be purchased with Milk
than without it.
Freguent Itemsets:
support

itemsets

0

0.84

(Milk)

1

0.70

(Bread)

2

0.60

(Butter)

3

0.60

(Egg)

4

0.54

(Cheese)

244

0.40

245

0.40

2486

0.40

247

0.42

(Banana,

Bacon,

Coffee,

Chicken)

248

0.40

(Pasta,

Banana,

Bacon,

Chicken)

(Pasta,
(Apple,

Butter,

Bacon,

Chicken)

Banana,

Coffee,

Chicken)

(Apple,

Banana,

Bacon,

Coffee)

Association Rules:
antecedents

consgeguents

support

confidence

lift

(Milk)

(Bread)

0.62

0.738095

1.054422

0

1

(Bread)

(Milk)

0.62

0.885714

1.054422

2

(Butter)

(Milk)

0.50

0.833333

0.992063

3

(Egg)

(Milk)

0.50

0.833333

0.992063

4

(Cheese)

(Milk)

0.50

0.925926

1.102293

Chicken)

821

(Pasta,

822

(Pasta,

823

{Banana,

Bacon)

Chicken)
Bacon)

824

(Banana,

Chicken)

825

(Bacon,

Chicken)

Ad .3 Machine learning approaches (HL)

(Banana,

0.40

0.833333

1.602564

Bacon)

0.40

0.740741

1.322751

Chicken)

0.40

0.714286

1.322751

Bacon)

0.40

0.769231

1.602564

Banana)

0.40

0.714286

1.552795

(Banana,
(Pasta,

(Pasta,
(Pasta,

@

=<

AINO TH

Python
import pandas as pd
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent patterns import apricri, asscciation rules
# Load data set

e e

df = pd.read csv('"biased transactions.csv")
# Generate frequent itemsets
frequent itemsets = apriori(df, minsupport=0.4, usecolnames=True)

e

# Generate association rules

rules = association rules(frequent itemsets, metric="confidence", minthreshold=0.7)
# Display the results
print ("Frequent Itemsets:")

e

print (frequent itemsets)
print ("Association Rules:")
"consequents",

"support",

"confidence",

"lift"]])

o

print{(rules|[["antecedents",

This example uses an algorithm called Apriori in the m1xtend library for discovering the
frequent data sets. llere is a high-level overview of the Apriori algorithm:
B Determine a threshold for the minimum level of support that will be considered (40 per cent
in the code example above).
B Identify individual items in the data set that meet the threshold and store their

appearance count.
B Progress to identifying pairs of items and larger groupings of items in the data set that also
meet the minimum threshold.
m If aset of items does not meet the threshold at a small level, it can be removed from
further consideration.
B The result is all combinations of items that appear frequently together at or above the
minimum threshold.

sasssssssnes

® See also
Reference the mlxtend documentation for assaciation rule processing by searching online for
mixtend documentation for association rule processing (https://rasbt.github.io/mixtend/user
guide/frequent_patterns/association_rules).
D

L

T T

P

ssssssssasssasans

DL
T T T T

Ty

A4.3.6 Reinforcement learning
Previously, “reinforcement learning” was described as learning from trial and error. In that
vein, it can be likened to a toddler learning to walk. Every time the toddler falls over, they

learn a little more about how to correctly balance themself next time, until they eventually
become a stable and confident walker.

A4 Machine learning

[

|

State & Reward

e

|

T
o
=

e

=)

-

Environment

-

Actions

—

J

M Reinforcement learning flowchart

As with most machine learning algorithms, reinforcement learning introduces some new
terminology to consider:

B Agent: The machine learning model that makes the decisions on what to do.
B Environment: The world, as perceived by the agent.
B State: A snapshot in time of the world. State is the data that communicates the current

situation or the environment. Careful consideration of your state data is critical when
developing a reinforcement learning algorithm. What data will you provide to the agent to

help it learn the task you have for it? For instance, when training an agent to play a snake
game, do you give it values indicating the distance and bearing of the apple, or a pixel map

of the entire world?
B Action: An operation or behaviour that the agent can perform in the environment (for
example walk forward, turn left, turn right).
B Reward: An immediate return from the environment in response to the agent’s action.
Reward may be positive or negative (a punishment).
m Policies: The strategies the agent will use to map states to actions. Think of policies as the
agent's mental if-this-then-thart list.
The general process is as follows:
B The agent will typically begin with a randomized policy, as it has no existing knowledge of
the environment.
B The agent observes the environment. Based on what it perceives, and the policy it has
recorded so far, the agent chooses an action to perform.
The agent performs the selected action.

The environment updates to a new state.
The environment provides feedback via a reward to the agent.
The agent updates its policy based on the reward feedback received.

The process repeats.
Reintorcement learning usually involves a combination of exploration and exploitation.
“Exploration” is when the agent ignores its learned policy and tries something new.

Ad .3 Machine learning approaches (HL)

@

ATNO TH

“Exploitation” is when the agent follows the learned policy and behaves according to what
it learned. Typically, an algorithm will start with a heavy emphasis on exploration, as the
algorithm hasn’t had much opportunity to learn anything yet. Over time, the hyperparameter
for the exploration / exploitation ratio, known as the “learning rate”, should adjust so as to

start deferring to the learned data more.

B Q-learning
The agent’s policies are responsible for maintaining what the agent has learned. While there

are a few approaches to this, one of the most common is known as “Q-learning”.
Q-learning can be thought of as using a 2D array or other data structure to create a giant
lookup table for every possible state and action combination. It stores a value for each possible

permutation of the two to predict what reward it would receive in each scenario.
Action 1

Action 2

Action 3

Action 4

State 1

—50

0

10

0

State 2

10

20

0

10

State 3

0

-10

0

50

The table illustrates a simplified version of a Q-learning 2D array. The data would suggest:
B When state 1 is seen, the best thing the agent can do is action 3, and it should avoid doing
action 1.

B When starte 2 is seen, the agent should do action 2, but action 1 and 4 would also give it
areward.
B When state 3 is seen, the agent should do action 4 for a large reward (possibly winning the
game), and avoid action 2.

A Q-learning table can be very large, given the array size is determined by all possible states
and all possible resulring acrions. When the data requirements are unfeasibly large, an
alternative approach is to use an artificial neural network to learn generalizations about the
state and resulting output actions. When a neural network is used, it is known as a “Deep
Q-Network™.
lere is a pseudocode overview of the process:
Initialize the Q-table with all zeros

(or some initial values)

for each round:
Initialize the state S to the starting point of the game
while the episode is not finished:
Choose action A from state S using a policy derived from Q
Take action A
Observe the immediate reward R and the next state S
Update the Q-table value for the original state S and action A:
Q(s, A)

<- Q(S, A) + alpha *

(R + gamma *
max(Q(s',

S

=-

# Move to the next

all actions))

- Q(S, A))

state

end while
end for

A4 Machine learning

There are a few comments to note about this pseudocode:
B When choosing action A, take into consideration whether the algorithm should exploit its
QQ-table or explore other alternatives.
B Alpha here refers to the learning rate. A higher alpha means that newer information has a
greater impact on updating the Q-values, allowing the agent to adapt quickly to changes in
the environment. A lower alpha will cause slower updates, making the agent more stable
but also slower to learn. A starting value between 0.01 and 0.05 would be normal.
B Gamma here refers to the discount factor for how much future anticipated rewards should
be considered when making a decision. The ultimate goal of most scenarios is to find
an optimal policy that provides the maximum cumulative reward. A gamma value close
to 0 makes the agent “myopic” (short-sighted), heavily prioritizing immediate rewards.
Conversely, a gamma close to 1 encourages the agent to consider future rewards more
strongly, valuing them almost as much as immediate rewards. This makes the agent “farsighted”, planning over a longer horizon.

m This line in the pseudocode is known as the “Bellman equation™
Q(S, A) <- Q(S, A)

+ alpha *

(R + gamma * max(Q(S', all actions))

- Q(s, A))
and it states that the Q-value for a state-action is equal to the immediate reward plus the
discounted value of the best action to take in the next state, adjusted for the learning rate.

B Example: Pong! game
The following example is a Pong!-style paddle-and-bouncing-ball game. It uses the pygame-ce
library for the graphics, and a numpy array for the Q-table. As can be seen in the results chart,
this agent requires about 20 minutes of training before it begins showing acceptable results.
Pong!: reinforcement learning results
1254

Nett reward

100+
75
50+
25

—25-4

—50+

0

T
500

T
1000

T
1500

T
2000

T
2500

T
3000

T
3500

Time (seconds)

M Pong! game

M Pong! game: nett reward over time

Python
import pygame

import random
import numpy as np
import matplotlib.pyplot as plt
# Constants

Ad .3 Machine learning approaches (HL)

@

z
o
2

=L

<

AINO TH

WIDTH, HEIGHT = 200,

400

FPS = 30

PADDLE WIDTH, PADDLE HEIGHT = 30,

15

BALL_RADIUS = 7

BALL COLOUR = (255,
PADDLE COLOUR =

255,

(255,

BACKGROUND COLOUR =

64,

(64,

64)
255)
64,

128)

# Initialize Pygame
pygame.init
()
screen = pygame.display.set
mode ( (WIDTH, HEIGHT))
clock = pygame.time.Clock()
class Paddle:

def

init_ (self, x, y):
self.rect = pygame.Rect(x,

def move(self,

y,

PADDLE WIDTH,

PADDLE HEIGHT)

x):

self .rect.x += X
self.rect.x = max(self.rect.x,

0)

self.rect.x = min(self.rect.x,

WIDTH -

PADDLE WIDTH)

def draw(self):
pygame.draw.rect (screen,

PADDLE COLOUR,

self.rect)

class Ball:

def

init_ (self, x, vy):
self.rect = pygame.Rect(x, y,
self.dx
self.dy

random.choice([-4,

BALL RADIUS*2, BALL RADIUS*2)
4])

random.choice
([-4, 4])

def move (self):

self.rect.x += self.dx
self.rect.y += self.dy
if self.rect.top <= 0 or self.rect.bottom >= HEIGHT:
self.dy = -self.dy

if self.rect.left <= 0 or self.rect.right >= WIDTH:
self.dx = -self.dx
def draw(self):

pygame.draw.ellipse (screen, BALL COLOUR,

self.rect)

# Game objects

paddle = Paddle (WIDTH//2 - PADDLE WIDTH//2, HEIGHT - PADDLE HEIGHT)

ball = Ball (WIDTH//2, HEIGHT//2)
# QO-learning parameters
LEARNING RATE = 0.05
DISCOUNT FACTOR = 0.59

epsilon = 0.1
# Let state have 10 positions for paddle and ball
#

(reduces demands on Q-Table)

def get_ state():

paddle mid = paddle.rect.x + PADDLE WIDTH // 2
ball mid = ball.rect.x + BALL RADIUS
return (paddle mid//20, ball mid//20)

A4 Machine learning

# QO-table

gtable = np.zeros((10,

10,

10 ball positions,

3 actions

3))

def update
g table(state, action, next state, reward):
# Bellman’s egquation
old value = gtable([state[0],

state[l],

action]

next max = np.max(g table[next state[0], next state[1]])
new value = (1 - LEARNING RATE)
LEARNING RATE *
q_table[state[0],

state[l],

* old value +

(reward + DISCOUNT FACTOR * next max)
action]

= new value

def choose acticn(state):
if random.random()

< epsilon:

# Explore: choose a random action
return random.randint
(0, 2)
else:
# Exploit:

choose the best action from Q-table

return np.argmax(q table[state[0],

state[1]])

def get_reward()
:
if ball.rect.bottom »= HEIGHT:
if ball.rect.colliderect (paddle.rect) :
return 1

# Reward for hitting the ball

else:
return -1
return 0

# Penalty for missing the ball

# No reward or penalty

running = True
nett = 0 # nett reward
cumulative =

T
o
=

=

# 10 possible paddle positions,

[]

# nett reward history for graphing

while running:
screen. fill (BACKGROUND
COLOUR)
for event in pygame.event.get():

# Use the quit icon for game termination
if event.type == pygame.QUIT:
running = False
# Agent decides and acts
state = get state()

action = choose acticn(state)
if action == 1:

paddle.move (-10)

# Move left

elif action == 2:
paddle.move
(10)

# Move right

# Update game state
ball .mowve ()
paddle.draw()
ball.draw()
# Check reward & state,
reward = get reward()

next state = get_state()

Ad .3 Machine learning approaches (HL)

update Q table

-

if reward = 0:
PADDLE COLOUR =

(0,255,0)

elif reward < 0:
PADDLE COLOUR =

(255,0,0)

# Update nett reward for graph
nett += reward
cumulative.append
(nett)

# Draw the game to screen
pygame.display.flip()
clock.tick (FPS)

pygame.quit ()
# Graph results

time =

[i/FPS for i in range(len(cumulative))]

plt.plot(time,

cumulative)

plt.title("Pong: Reinforcement learning results")
plt.xlabel ("Time

(seconds)")

plt.ylabel ("Nett reward")
plt.grid(True)

plt.show()

@® See also
Gymnasium is a popular library for learning about reinforcement learning. It provides a number
of pre-written environments that you can experiment with: https://gymnasium.farama.org

sessssssssnns

AINO TH

update
g table(state, action, next state, reward)

RO

A4.3.7 Genetic algorithms
4 Genetic algorithm:
imitates the concept of
survival of the fittest and
evolution by testing a
population of possible
solutions to a problem,
using properties from
the best-performing
solutions to create a
new population of
possible solutions, and
then repeating the
process until a suitably
performing solution has
been identified.

Genetic algorithms are not normally classified within the traditional categories of “supervised
learning”, “unsupervised learning” or “reinforcement learning”. They are considered an
evolutionary algorithm. They learn through a process of optimization inspired by the process
of narural selection.
A high-level overview of the algorithmic process is:
m Start with a population of possible solutions to the problem. This may be in the form of an

array of strings, for instance, where each string is a randomly generated possible solution.
B Each item within the population is evaluated through a fitness function that returns a metric
for how good the possible solution is.
W Fairs of solutions are selected for reproduction. Various algorithms will be discussed that
make these selections, but generally the better the fitmess function score, the more likely a
solution is to be selected.
B The selected pairs then undergo reproduction using a crossover algorithm, where part of
the genetic code of each parent is selected and then combined together to create a new

possible solution.
B The new offspring may then undergo mutation. Random number functions are generally
used so that only a small percentage of offspring undergo mutation, and then those selected
have parts of their genetic code (their “solution” to the problem) randomly altered.

A4 Machine learning

B Once a new generation of offspring has been generated, they become the current generation,
and the process of calculating fitness, selection, reproduction and mutation repeats itself.
B The process repeats until whatever termination criteria you determine is satisfied. This

might be to iterate for a given number of generations, or to iterate until a fitness score of a
minimum threshold has been reached.
Genertic algorithms are used for problems where it is not essential to identify the most perfect,

optimal solution, but where there is a degree of “close enough is good enough” flexibility.
Common example applications of genetic algorithms include:
B Route planning, such as the travelling salesperson problem. Consider the scenario of a

salesperson who has 50 cities to visit. Rather than crisscrossing the countryside, genetic
algorithms can find a close-to-optimal route to minimize the distance travelled. In the
context of a long journey to 50 cities, it is not necessary to find the most perfect solution,
so long as the solution is good enough. Put another way, if the algorithm can find a good
solution in a few minutes, is it really worth hours or days of additional processing to find a
solution that might be only 1 per cent better? At some point, the route is good enough to use.
B Timetabling, such as allocating students to their preferred classes where there are
constraints on the number of teachers, rooms and classes available.

m Civil and mechanical engineering to help optimize design of structures such as bridges or
buildings, and vehicle designs to make choices of materials based on durability, strength

and cost.
B Control systems and robotics use genetic algorithms to optimize the controller for better
stability and performance.

m Finance applications use genetic algorithms to help optimize a trade-off between risk and
reward in selection of an investment portfolio.
B Within machine learning, a genetic algorithm can help select a subser of relevant features

from a larger data set to improve model accuracy and reduce overfitting.
B Insome types of machine learning problems, it is possible to use genetic algorithms for
the training of an artificial neural nerwork as an alternative approach to backpropagation
(see later in this section).

B Selection functions

wheel is rotatey

Selecting pairs of values for reproduction relies on the
concept of weighted randomization. It is weighted in the
sense that those with higher fitness scores should be more

likely to be chosen for reproduction. Randomization is still
important, however, to ensure that the algorithm doesn’t

become trapped in a local maximum.

selection
point

One of the most commonly used approaches is the
concept of a roulette wheel, where the portion of the wheel

allocated is determined by the [itness function score.

the fittest individual
has the largest share
of the roulette wheel

the weakest individual

e has the smallest share
of the roulette wheel

M Roulette wheel selection

Ad .3 Machine learning approaches (HL)

z
o
Z

I

=<

ATNO TH

H Crossover functions
Crossover functions define the algorithm used for reproduction — taking two input solutions

and mixing them in such a way as to produce a “child”, representing a new valid solution.
There are a variety of common algorithms to do this, but each needs to be considered in the

context of the problem. Any approach will likely need to be tweaked to ensure the “child”
created through the process is valid for the scenario.
ol1Tol1T11Tol1To

o111 lolol1To

Two common methods are the one-point crossover and the
two-p olnt crossover.

One-point crossover selects a random point in the gene
sequence to slice the data. The data from up until the slice
ol11ol111

0

ol1111111

point is copied from parent 1 into child 1, and then the rest
1]0/1]0

of child 1's data comes from parent 2. The inverse can also
occur at the same time to create a second child.
Two-point crossover works in a similar manner, except two

B One-point crossover

slice points are selected.
There are other commonly used methods as well, but

0[1]0]1]1]1]0|1]0

0/1]1]1]1]0|0|1]0

what matters
most is that you are using a randomization

function to create offspring that are a mix of the data from
A

two paren[s‘

tip!
[O[1[1[1T1T1]o 110 (®Top

o[1o[7T1jolonl0l

Choose appropriate genetic operators (selection, crossover,
mutation) for your problem, and ensure diversity within the
population to avoid premature convergence.

M Two-point crossover

B Example: Travelling salesperson
Route travelled

Route travelled

1000

1000

800

800

600

600

400

400

200

200

0
0

T
200

T
400

T
600

M Travelling salesperson — random route

T
800

T
1000

0
0

T
200

T
400

T
600

T
800

T
1000

M Travelling salesperson — optimized route

Consider the charts to represent a map of 50 cities that a travelling salesperson wishes to
visit, with two possible routes for the journey. Clearly, the randomized journey is inefficient,
whereas the optimized journey is a lot more efficient.

A4 Machine learning

Two questions to ponder: Is the optimized route perfect? And, does it matter?

=
o
=
ze

To find the absolute most perfect solution would require testing 50! permutations; that is, over

30,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000
permutations. That is a lot of processing! The reality is, in scenarios such as this, rather than

-

labouring for perfect, good enough will do. In the context of travelling berween 50 cities, does

it make sense to spend exponential time in calculations to save just a few minutes of travel?
This is an example of the type of problem that genetic algorithms can help solve.

Step 1: Create an initial population
Assign each city a number and use a random number generator to create a randomized route.
Do this for an initial population of 500 possible solutions (population size being one of the
hyperparameters you may want Lo tune).
r

1
1

|
1
1

b

1
1

Python

1

def create_random route():

1
1

I

route =

:

random.shuffle (route)

:

:

return route

:

[n for n in range(0,50)]

1

:

: def create initial population():
return

:

[create random route()
—

for n in range (0,

1
1

500)]

[

Ll

Step 2: Create a fitness function to measure the performance of each
member of the population
The obvious metric is to use the journey distance of the route; however, given the crossover
function will give preference to those with a higher score for reproduction, it is necessary to
use a fitness function that gives the highest scores for those routes with lowest distances.
One simple solution might be to calculate the fitness score by setting the distance as a fraction
denominator, such as:

fitness, = —

distance,

An alternative approach used in the example code that follows is to use normalization to
convert the data into a [0,1] range, where the 0 represents the highest distance and the 1
represents the smallest distance. The resulting normalized values are then used as an exponent
to ensure that small differences in the normalized value, especially at the higher end, caused
very large differences in fitness.
.

normalizedi =

distancemax __— distance,f

distance_— distance,
;
=
100+normalized;
fitness =2
i
The selection function is yet another hyperparameter for you to tune and experiment with for
each given problem. The method outlined here was only selected after experimenting with half
a dozen different possible approaches to see which produced the quickest results.

Ad .3 Machine learning approaches (HL)

@

AINO TH

Python
def calc distance(route):
dist = 0

for i in range(l,

len(route)):

# Get the co-ordinates for each pair of cities
xl,yl = coords[ route[i-1]
xX2,y2 = coords[ route[i]

]

]

dist += math.sgrt( abs(x1-x2)**2 + abs(yl-y2)**2

)

# Don't forget to return home at the end!
x1l,yl = coords|[ route[len(route)-1]
x2,v¥2 = coords[ routel[0]
dist += math.sqgrt(

]

]

abs(x1-x2)**2 + abs(yl-y2)**2

)

return dist
def calc fitness(distances):

max_distance = np.max(distances)
min distance = np.min(distances)
normalized =

(max_distance-distances)/ (max_distance-min_distance)

fitness = np. power (2.0,

(normalized*100))

return fitness

Step 3: Reproduction using a crossover method
In this case, the code below is using a modified form of one-point crossover. A point in the
data is randomly selected, and the genetic data from parent 1 is copied across without change.

After that, the remaining cities are copied over from parent 2 in the order in which they
appeared in parent 2.
Here is a simple example with six cities:

PARENT 1

[0,4,5,1,3,2]

PARENT 2

[4,2,0,3,5.1]

Randomly decide how many genes to copy from parent 1. In this case, use three.

CHILD

=10,45 _,_, _1

Now copy the remaining values from parent 2, skipping the values already present from parent 1.
CHILD

=10,4,52,3,1]

Python
def reproduce (parentl, parent2):
# First set of genes will come from parent 1
count _of genes fromparentl = random.randint
(0, len(parentl))
child = parentl[0: count of genes fromparentl]
# Remaining genes will come from parent 2
for i in range (0,

len(parent2)):

# Only include genes from parent2 not already provided by parent 1
#

(don't want to visit the same city twice)

if parent2[i]

not in child:

child.append(parent2[i])
return child

A4 Machine learning

Step 4: Mutation
There is no fixed algorithm to use for mutation; it will vary somewhat depending on the

z
o
=

I

context of your problem. In this case, the following code will, in 5 per cent of cases (the

mutation rate hyperparameter), randomly pick a pair of cities in the route and swap them.

=<
1
1
1
1
1
1

Python
def mutate(person, mutation rate):
if random.random()

< mutation rate:

a = random.randint (0,
b = random.randint (0,

personlal, person[b]

len(person})-1)

:
1
1
1
1
1
1

len(person)-1)

= person[b], person[a]

return person

a

Step 5: Promote the children to be the active generation
This will be just a one-line task at the end of the loop to copy the data from the new generation
that was being produced, into the array being used for the active generation.
r

b

' Python

:

1

1

: # Move to the next generation

:

I population = next generation
1
-

1
1

=

Ll

Step 6: Write the main function to bring it all together

Python
import math,

random,

json

import numpy as np
with open("travelling-salesperson.json","r")

as f:

coords = json.loads (f.read())

def travellingsalesperson(population
size=500, generations=5000):
# Create randomized population
population = create_initial population(population size)
minimum = 50000
generation number = 0
while minimum > 5000 and generation number < generations:
# Calculate fitness for each person

generation number += 1
distances = np.array([calc distance(population[n])

(0,

population size)])

for n in range

N

fitness = calc_fitness(distances)

if generation number % 25 == 0:

print (f"Generation {generation number}: Best {calec distance
(population[ np.argmax(fitness)

])} Mean { distances.mean()

}|")

# Create the next generation

next generation =
e

[]

e e e e e e e

Ad .3 Machine learning approaches (HL)

e e e e e e

e e

e

e R M M e e e

A

A e e e e e e

e e

@

AINO TH

for p in range (0, populaticn size,

2):

# Select parents

# Select k=2 items from "population", using the values in "fitness"
# to determine probability weighting.
parents = random.choices (population,

weights=fitness,

k=2)

# Reproduce
childl = reproduce (parents[0], parents[1])
child2 = reproduce (parents[1],

parents[0])

# Mutate
childl = mutate(childl,

0.05)

child2 = mutate(child2,

0.05)

next generation.append(childl)
next_generation.append(child2)
# Move to the next generation

population = next generation
# All done.

What are the results?

fitness = np.array([calc fitness(populaticn[n])
(0,

for n in range

population size)])

best = np.argmax(fitness)

print (f"Best person after {generations} generations is #{best}")
print (£"Their travel distance:
print (f'"Their route:

{calc distance (population[best])}")

{population[best]}")

travelling salesperson()

Travelling salesperson

As you let that execute, it will print an update every

25 generations with the progress it has made, similar to the
220001
-

following update:

20000

.

Generation 25:

Best

13496.55 Mean 14238.29

Generation 50:

Best 11301.76 Mean 11541.97

c

,,:Oj 18000
@

g 16000

—

Generation 75: Best 9265.94 Mean $365.81

& 14000

Generation 100: Best 8944.63 Mean 9033.31

E 12000

Generation 125:

-

10000

Best 8727.39 Mean 8883.68

This chart shows the minimum travel distance calculated

8000
T

T

0

1000

T
T
2000
3000
Generation

T
4000

"=
T
5000

M Best route found for travelling salesperson for each generation

after each generation when the algorithm was executed by
the author. There are a couple of important aspects to draw
.
your attention to.
Firstly, and most obviously, is that most of the
improvement in output occurs very quickly, after which the

law of diminishing returns starts to apply.
The second thing to note, though, is that the output will frequently get stuck in a local
minimum for several hundred generations before suddenly breaking out of it, so there can be
benefits to gain by being patient enough to give that the opportunity to occur.
What is the lowest distance route you can obtain to visit all 50 cities?
Download the data file from https://github.com/paulbaumgarten/hodder-ibdp-computerscience

A4 Machine learning

ererceptron: thedata | A4.3.8 Artificial neural networks
structure at the heart
of an artificial neural

An artificial neural network (ANN}) is an algorithm thar learns to make decisions by finding

r:E
g

network; it represents a
single artificial neuron
that takes in multiple
inputs and weights,
and generates an

patterns in data using an approach modelled on the biological brain. Just as a biological brain

[

output value.

commonly referred to as “neurons”.

Input layer

consists of many neurons that are interconnected and send signals to each other via synapses,
so too an artificial neural network is an algorithm that defines a series of nodes that transmit
data between each other. These nodes are known as perceptrons, though they are also very

¢

Hidden layer

Output layer

The diagram represents a typical structure for an ANN:
® The input layer receives the input values that the
network is being asked to process. Each feature of your
model requires its own input perceptron. The value
given to each perceptron is numeric (either an integer
or float, depending on the problem context).
B There are usually one or more hidden layers within
an ANN. These are layers of perceptrons that identify
patterns in the input data to make generalizations useful
for the next layer. They receive the values from the
previous layer, perform their calculations and then send
their respective result to the next layer in the network.

Artificial neural networks

B The output layer is where the ANN produces a final

M Layers in an artificial neural network

“answer” value or prediction.

The illustration in this case is also an example of a fully connected network, in that every
perceptron from one layer is connected, and sends its output value to, every perceptron in the
next layer. Fully connected networks are the norm.

B A single perceptron
Zooming in on a single perceptron, the following sketch outlines the different elements
at work:

(Bias)

1

@%\
(Inputs) —

@,,,WL,”'
s

Yored
(Activation function)

w,
(Summation function)
(Weights)

B A perceptron in an artificial neural network

® Input: The perceptron receives an input value from every perceptron in the layer before.
B Weight: Every input has a weight associated with it. The weight is a value indicating the
importance this particular neuron places on the values from the respective input. Weights
are usnally initialized with a random number between —1.0 and 1.0 and then adjusted by
the training process.
Ad .3 Machine learning approaches (HL)

@

Bias: Supplemental to weights, a typical neuron would also have a value called the “bias”.
This is added to the value from the summation step, prior to using activation. The bias acts

as a way to shift the decision boundary along the curve of the activation function. The bias
is usually initialized with a random number between —1.0 and 1.0 and then adjusted by the
training process.
Activation: The activation function helps determine whether or not the neuron should be
“active” (*inactive” in this case means the neuron would have an output value of 0). The

activation function serves to introduce nonlinearity to make neurons more expressive.

That is, it helps force the neuron to make a decision. For example, one commonly used
activation function is ReLU, which results in a neuron being active for any positive value,

and inactive (0) for any negative value. A comparison of common activation functions
follows later in this section.
Output: Finally, the resulting value returned from the activation tunction is sent onward to
the neurons in the next layer, or the external system.
The following example is a walkthrough of the calculations for a perceptron:

Q9P

ATNO TH

Summation: The product of each input value and its respective weight are summed together.

-3.1

1.6.

9.98

2.9

4.78—>

478>

2.7

W Example values in a perceptron

The perceptron receives input values of 1.3, 4.2, 0.0 and 2.7.
Each input path has a weight of 3.1, 1.6, 2.9 and 2.7 respectively.

Each input and its respective weight are multiplied, and the results added together:
(13+-31+#2=16)+(0.0=29) + (27« 27) =998
The bias value is added, which, for this example, is -5.2:

008 +(-5.2) =478
The resulting value is passed through the activation function, which, in this case, is ReLU:

RelLU(4.78) = 4.78
The output value 4.78 is passed along to the next layer in the network, or is given as the
output value of the network, if it is the output layer.
From a mathematical perspective, up until the activation function, the rest of the perceptron

can be considered a linear function, where the weights are the variable coefficients and the bias
is the constant:
¥ =RelLU(xw, +x,w,+x W, +xXW, +b)

Or, to express it more generally:
y = activation ((i‘,xiwi) + b)
i=0

A4 Machine learning

Given the output of any individual perceptron can be expressed as a function, and that the
inputs of perceptrons are either input values or the outputs of other perceptrons, it means that

=

the entire artificial neural network behaves as a function.

g

(-

q

# Activation function:
a mathematical function

® Common mistake
Qvercomplicating the model architecture can lead to overfitting and high computational costs.

applied to the output of

Start with a simple architecture and gradually increase complexity, if necessary.

a neuron that is used to
determine whether or

not the neuron should
be activated (considered

B Activation functions
While there are a large variety of activation functions in use, there are four that are more

to be "on”).

common than all others: ReLU, Sigmoid, Softmax and tanh.

.

RelLU function

RelLU

ReLU (rectified linear unit) is often the default choice for
ANNs. Tt is computationally efficient and is less likely

4-]

to have a vanishing (approaching zero) gradient, unlike

%3

Sigmoid or tanh. The function for ReLU is:

3

() = max (0,%)

‘32_
3

On first impression, it may appear that ReLU is linear;

&

however, it is more accurate to say it is two different lines
i

coming together in the one function, one on the positive

0

side, and another on the negative. The simple act of the
zeroing of negative values significantly changes the
_A

_'2

(')

2'

a

neurons will be activated and any negative-value neurons
will be deactivated. This small change makes a big
difference when attempting to do classification problems.

Sigmoid function

Sigmoid

M The ReLU function
194

behaviour within a network as it means that only positive

Input value

Sigmoid is commonly used in the output layer for binary
109

classification problems since it maps to a distribution

= 0.8

between 0 and 1, which is generally what is desired at

“;

the output layer. It is used in scenarios like email spam

‘g 0.6

detection (spam or not spam) and medical diagnosis (sick

2 0.4

or healthy). It is not usually used in hidden layers in deep

E

networks due to their vanishing gradients. Observe that

@ 0.27

once the input value is less than —4 or greater than +4, the

0.0

gradient becomes so insignificant it might as well be zero.

The equation for Sigmoid is:
-0.2

|

-4

T

-2

T
0
Input value (x)

2

4

1

j(x) T leer

M The Sigmoid function

Ad .3 Machine learning approaches (HL)

@

1

7
e

2

e

e
]
|

Probability

e

=

=

©

ATNO TH

Softmax probabilities for varying logits of Class 1

0.0

Key
~— Class 1 probability
—— Class 2 probability
— Class 3 probability
~— Class 4 probability
~— Class 5 probability
~— Class 6 probability
~ Class 7 probability
~—— Class 8 probability
~ Class 9 probability
~ Class 10 probability

S

T
T
-10.0 -75

Softmax produces an output similar to Sigmoid in that
both produce values in the range (0,1). Softmax produces
a probability distribution for N different outcomes,
where N is the number of categories for classification

and the probabilities sum to 1. This makes it suitable for
distribution across multiple classes, so it is commonly used

for the output layer of a multiclass classification problem.
(In the chart, the lines for Classes 2 to 10 are aligned and

7 e .~ L R
T
5.0

1
1
1
-25 0.0
25
Logit for Class 1

M The Softmax function
1.5

Softmax

stacked one on the other, which is why it appears as if only
two lines are plotted.)

Tanh
Tanh is similar to Sigmoid, but the output values range
between —1.0 and 1.0. It is useful when your data is

Tanh function

normalized around 0 but, like Sigmoid, it also has
vanishing gradients that can be problematic. It is more
common to see tanh used for hidden layers than Sigmoid,
around 0 makes learning for the next layer easier for

o
o
1

classification.
There are a couple of mathematically equivalent ways of
producing the tanh function:

(e —-e™
fo = (e*+e™)

L|
o
|

Tanh output (y)

given its mean distribution is centred on 0. This centring

|
N
5

or

2

Input value (x)

0= e !

M The tanh function

B Generating a prediction
Using an ANN to generate a result is a matter of performing all the calculations on all the
perceptrons in one layer, and then feeding forward those results to the next layer. The process
continues until the output layer is reached and the process terminates.
While overengineered for the scenario, imagine using a neural network to determine the result

of OR and AND logic gates. Consider the following network with two input neurons, four
neurons in one hidden layer and two output neurons.
After training (discussed in the next section), the network consists of the following weights
and biases. The ReLU activation function is used on the hidden layer and, since the network is
seeking to perform a classification task, Sigmoid is used on the outpur layer.

A4 Machine learning

=
o
Z
25

OR

'bias 0.2
AND

bias -0.84

bias -0.23

W Example values for a logic gate ANN

For those unfamiliar with logic gates, see Section A1.2.3. The network should produce the
following results, if behaving correctly:
A

B

OR AND

[0, O]

->

[0,

0]

[0, 1]

->

[1,

0]

[1, o]

->

[1,

o]

[1, 1]

->

[1,

1]

Performing the calculations for an input of [1, 01, the following occurs:
Hiddenl = ReLU( 1.00 * 0.60 + 0.00 * -1.13 + 0.53 )

= ReLU( 1.13 )

= 1.13

Hidden2 = ReLU( 1.00 * -0.47 + 0.00 * -1.11 + 0.00 ) = ReLU( -0.47 ) = 0.00
Hidden3 = ReLU( 1.00 * 0.70 + 0.00 * 2.10 + 0.00 )
Hidden4 = ReLU( 1.00 * 2.10 + 0.00 * 0.80 - 0.23

)

= ReLU( 0.70 )

= 0.70

ReLU( 1.87 )

= 1.87

Now, use these hidden layer values to generate the output values.
Qutl

Sigmoid( 1.13%-0.25 + 0.00%0.94 + 0.70*%1.73 + 1.87*0.71 - 0.26 )
= Sigmoid( 2.00 )
= 0.88

Out2 = Sigmoid( 1.13*-1.48 + 0.00*-0.69 + 0.70*%-0.13 + 1.87*0.77 - 0.84 )
= Sigmoid(

-1.16

)

0.24

Since our classification problem is seeking a 0 (“false” or “no”) or 1 (“true” or “yes”) answer,
when 0.88 and 0.24 are rounded, the network has indeed correctly determined that an input of
[1,0] into an OR gate results in a 1, and [1,0] into an AND gate results in a 0.

@ See also
: See 3BluelBrown's YouTube video “But what is a neural network?”

.
R
P T T Ty

Ad .3 Machine learning approaches (HL)

@

=<

ATNO TH

B Training
While calculating an output result or prediction from a neural network should be a
conceptually straightforward mathematical process, the training process is more complex and

is far beyond the scope of your course.
The process used is known as backpropagation. As a high-level overview, here is what

# Backpropagation:
backpropagation of
errors is the most
commonly used
technigue for training
artificial neural
networks. The gradient
of the loss function is
calculated, and used
to update parameters
such as weights, in the
opposite direction of the
gradient to reduce the
overall error.

is occurring:
B We calculate the error in the output values received from the network when compared to
the target output values in the training data. A loss function is used for this, such as “meansquared-error” for regression, or “cross-entropy loss” for classification tasks.
B We calculate how much each parameter (the weights and biases) in the network
contributed to the error. This is done by using the gradient (i.e. the derivative or slope) of
the loss function for each parameter.
B An optimization algorithm such as “gradient-descent” is used to calculate adjustments to
the parameters. By knowing the gradient of the error, the parameters can be adjusted in the
opposite direction (gradient descent) to reduce the loss (see the videos referenced in the
“See also” box).
® Before applying the adjustment to the weights and biases, we multiply them by the learning
rate hyperparameter. This is to ensure we don't overcorrect and solely design the network
around any one particular value in the training data set.
B Once this process has completed for one layer (such as using the output layer to calculate
adjustments to the last hidden layer), we repeat the process on the layers before it. This
process of moving backwards from the output layer, working through each hidden layer,
until finally reaching the input layer, is where the term “backpropagation” comes from.
B We repeat the entire process a certain number of iterations or until the loss stops
decreasing significantly. Each pass over the data set is known as an “epoch™
LR

R

TT
e
P RER

sssssssanssans

R

ssssssans

® See also
See 3BluelBrown'’s YouTube videos “Backpropagation, step-by-step | DL3" and “Gradient
descent, how neural networks learn”.

D
LT T P PP

B Example 1: Logic gates
This is the Python code used for the OR and AND logic gates example in the
previous walkthroughs.
e

e

e

e

e

il

E Python

L

|

E

:

import tensorflow as tf

I

from tensorflow.keras.models import Sequential

1

:

from tensorflow.keras.layers import Dense

:

:

import numpy as np

:

# Inputs:

[A,

: %x = np.array([[0o,
# Outputs:

:

y = np.array([[0o,
o o o

e e

:
:

B]

:

o

:

[OR,

0],

[0,

11,

[1,

0],

[1,

111,

dtype=float)

:

AND]
0],

:
[1,

O],

[1,

0],

[1,

1]1],

dtype=float)

:

e e e e e e e e R e e e R e e e e e e R e e e e R e e R e e e e R e R e e e R e e e e e e e e e e e e e e e

e e o

A4 Machine learning

# Define and compile the model

o
Z

==

model = Sequential
([

# Hidden layer with 4 neurons,
Dense (4,

input dim=2,

# Output layer with 2 neurons,
Dense (2,

using RelLU

-

activation="relu"),
using Sigmoid

activation="sigmoid")

1)
model.compile (loss="binary crossentropy",
model
. fit(x,

y,

epochs=1000,

optimizer="adam",

metrics=["accuracy"])

verbose=1)

# Making predictions
predictions = model.predict
(x)

print ("Predicted ocutputs:\n",

predictions)

# Evaluate the model

loss,

accuracy = model.evaluate(x,

print ("Accuracy:

y)

{:.2f}".format (accuracy))

# Print weights and biases for our curiosity
for layer number,
weights,

layer in enumerate (model.layers) :

biases = layer.get weights()

print (f"Layer {layer number+1}")
print ("Weights:\n",

weights)

print ("Bizses:\n", biases)
print ("\n")
e

e e

R

e

e e

e

e e

e e e

e

e e

e e e

e e e e

e e

e e

e e e e

e e e

e e e e

e e e e

e e o

( ® Common mistake
If you are installing TensorFlow on a computer without a GPU, ensure you install a CPU-only version
otherwise you will receive errors that pip is unable to find a version that satisfies the requirements.
That is, from your terminal, run the following:
pip install tensorflow-cpu
For more detailed instructions, refer to the TensorFlow installation guide at
www.tensorflow.org/install/pip

H Example 2: ANN for regression
A commonly used example for introducing regression problems with an ANN is the California
housing data set. It contains information about various homes in Calitornia in the 1990s,
including such features as house age, average number of rooms, average number of residents,

and latitude and longitude, and is used to predict house prices.
More information on the data set can be found by searching online for Keras California

Housing price regression data set (https://keras.io/api/datasets/california_housing).
1
1
1
1
1
1
1
1
1
I
1

Python
import tensorflow as tf
from tensorflow.keras import layers,

models

from sklearn.model selection import train test split
import numpy as np

Ad .3 Machine learning approaches (HL)

@

AINO TH

# Load data set
(x,

¥),

(xtest,

y test)

= tf.keras.datasets.california housing.load data(

version="large", path="california housing.npz",

test split=0.2, seed=113

)
# Split the data into training and validation sets

# xtrain is the training data, ytrain is the training labels
# xval is the validation data, yval is the validation labels
X _train,

x val,

y train,

r;ndomistagezo)

y val = train test splitix,

B

B

# Define the network,

-

compile,

B

vy,

test size=0.2,

B

and train it

medel = models.Sequential
([
layers.Dense
(64, activation="relu",

input shape=(x_train.shape[1],)),

layers.Dense
(64, activation="relu"),
layers.Dense
(1)

1)
mcedel .compile (optimizer="adam",
loss="mse",

metrics=["mae"])
history = medel.fit(x train, ytrain,
epochs=100,

validation data=(x_val, y wval))
# Run the unseen test data through the network to determine success
test loss, test mae = model.evaluate(x
test, y test)
print (f"Test data - mean-absclute-error:

S

IR

B

AL

I

A

L

e
4

¢

4 4

4

¥ N

4

4

v

0
3

5 §F &£ 5 5 5§
G 6 L 6 6 b &
77 77r?r7 2

5§55
¢ ¢ ¢
777?

§

¢+

#

9 ¢

r

*

B

729 %

8

¢

279

B Example 3: ANN for classification
A very common Hello World-style classification problem for ANNs

2 2 )
a2 20
F 3 2 33 7523 3
4

{test mae}")

8

is the MNIST number recognition data set. It comprises 60,000
28x28 grayscale images of the ten digits, along with a test set of
10,000 images.

¢

9729

M Examples of handwritten digits in the MNIST data set

Python
import tensorflow as tf
from tensorflow.keras import layers,

models

from tensorflow.keras.utils import to_categorical
import matplotlib.pyplot as plt

import numpy as np
# Load data set
(x_train,

y train),

(x test,

y test)

= tf.keras.datasets.mnist.load
dataf()

A4 Machine learning

oy
A
R R M
R R R

# Convert grayscale pixels into floats with range

1
1
]
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
:
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1

[0...1]

x_train = x_train/255.0
xtest = xtest/255.0
# Convert the labels into "1 hot encoding" category arrays
# e.g.

label of 3 becomes

[0,0,0,1,0,0,0,0,0,0]

y_train = to_categorical (y_train)
ytest = to categorical(y
test)
# Define the network,

compile,

and train it

model = models.Sequential
([
layers.Flatten(input shape=(28,28)),

layers.Dense
(64, activation="relu",

input shape=(x train.shapell],)),

R

R R

layers.Dense
(64, activation="relu"),
layers.Dense
(10, activation="sigmoid")
1)
model.compile (optimizer="adam",
loss="binary crossentropy",
metrics=["accuracy"])
history = model.fit(x_train,

y_train,

epochs=5)

# Run the unseen test data through the network to determine success

loss,

accuracy = model.evaluate(x
test, y_test)

print (f"Accuracy:

{accuracy}")

:

P

model .save("mnist-example.keras")

-

One step that may not be intuitively obvious is how to test this with your own dara. Suppose
you have a 28x28 grayscale PNG file youd like to test on the ANN. The following is example

code to do this task. (By the way, ensure your image is white text on black background, as that is
how the model has been trained.)
r

1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
:
1

-

Python
import numpy as np
from PIL import Image

# pip install Pillow

import tensorflow as tf
model = tf.keras.models.load
model ("mnist-example.keras")

# Load the image file
image = Image.open("your image file.png")
image = image.convert("L")

# Grayscale to match the training data

image = image.resize((28,

28))

# Resize to match the training data

# Convert image to numpy array

1
I image array = np.array(image)

: image array = image array/255.0 # Normalize to 0..1 scale
: # Reshape the array for the model

(Add a batch dimension at the beginning)

: image array = image array.reshape(l,

28,

28)

| # Send to the trained ANN
: predictions = model.predict(image_array)
:

predicted class = np.argmax(predictions,

axis=1)

1 print ("Predicted class:", predicted class)
1

Ad .3 Machine learning approaches (HL)

1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1

@

o
Z

==

-

ATNO TH

A4.3.9 Convolutional neural networks
# Convolution: a
mathematical operation
that combines two
functions to produce
a third function.
In the context of a
convolutional neural
network being used
for image processing,

convolution applies
filtering functions to the
pixels in an input image
to compute distinctive
features from the data.

A convolutional neural network (CNN) extends on the architecture of ANNs by using
additional layers of calculations prior to processing the data through a fully connected
artificial neural network.

CNNs are ideally suited to processing image data but are also valuable for applications such
as video analysis, natural language processing (NLP), audio and speech processing, and

recommendation systems.
To explore what makes a CNN different from an ANN, consider the following diagram of a
typical convolutional neural network:
Feature maps

Convolutions

Subsampling

Convolutions

Subsampling Fully connected

M Structure of a convolutional neural network

B Input layer
The input layer comprises the raw pixel data of the image being processed. The number of
input nodes would be based upon image width x image height x colour depth.
It is important to consider that, when dealing with input data such as images, the input data
can get very large with relative ease. A small image of only 100 x 100 pixels at full colour
resolution (that is, 1 byte each for red, green and blue) would comprise 300,000 values. This is

too much to feed directly into an ANN, and so preprocessing through convolution and pooling
is used to reduce this to something more manageable.

B Convolutional layer
The convolutional layers serve as [eature extractors that are looking for patterns in the
input image.
As the network trains, it develops filters (also known as “kernels”) that learn to detect
patterns that are important for the individual network at hand. Generally, these patterns are
as simple as edge detection or various textures, but they can be used to detect more complex
shapes within the image. Other common patterns that may emerge are sharpening filters and
blur filters.
Edge detection will typically look for vertical or horizontal edges. For instance, the matrix to
find vertical edges may look like this:
-1
-1
-1

0
0
0

1
1
1

Applying an edge detection filter to an image is shown in the following diagram. The first
image is the original; the second is the output using vertical edge detection with the matrix
above; the third is horizontal edge detection after rotating the matrix clockwise.

A4 Machine learning

L

==

[}

E

-

M Kitten bitmap with its horizontal edges and vertical edges detected

A sharpening filter will emphasize the difference in contrast between adjacent pixel values.
This helps make the image look more distinct to later stages of the network. The matrix for a
sharpening filter may look like this:
0
-1
0

-1
5
-1

0
-1
0

A blur filter will help reduce noise and detail from an image, so the network doesn't focus on
minor variations of detail within the image that do not carry significance in meaning. For

instance, a gaussian blur may be applied through the following matrix:

"1/16 1/8 yﬂ
8 14 18
/16 1/8 1/16
The strength of CNNs lies in their ability to learn the most appropriate filters for a given task
through backpropagation during the training process. The initial values for these kernels can

be set randomly, or by using some heuristic, and then be updated to better suit the specific
features of the training data.
The act of iterating over groups of pixels with a filter is known as “sliding” or “striding”.

@ See also
-

§ See 3BluelBrown's YouTube video “But what is a convolution?”

H Activation function
Convolutional operations are linear transformations. Mathematically, they are the dot product
between the filter (kernel) values and the pixel values in the image. If you stack multiple
convolutional layers, the entire network can still be described as a single linear transtormation.
No matter how many layers you have, they can still be collapsed into a single layer that
performs one linear transformation, because the composition of two linear functions is still
a linear function (the function for any one pixel can be reduced to the sum of input values

multiplied by various fixed coefficients).
This poses a problem for our network, as a linear system can only perform linear classification.
That is, it can only separate data using a straight line (or hyperplane in higher dimensions).
‘While this has its uses, the tasks we generally require of CNNs are too complex for a linear

approach to separation.

Ad .3 Machine learning approaches (HL)

ATNO TH

Therefore, after convolution, the data is run through an activation function to introduce non-

linearity to the data.

Bl Pooling layer
The next stage of the process is to pass the data through some down-sampling layers, also
known as “pooling layers”. The pooling layers serve to reduce the dimensions of the image data.
This serves a couple of important purposes. Firstly, it will reduce the number of parameters

that need to be input into a deeply connected network, significantly reducing the
computational workload of the network. Secondly, it also assists with network learning, as it

will further help negate minor changes in individual pixels (image noise) that are unlikely to
be relevant to image classification. This helps to reduce the risk of overfitring.
There are two commonly used methods of pooling:
B Max pooling, which takes the maximum value from a set of values.
B Average pooling, which takes the average value from the set of values.

B Fully connected layer
After pooling, the data is then fed into a fully connected artificial neural network (ANN),
which has been previously discussed.
The purpose of the ANN at this point is to take the high-level features learned by the
convolutional and pooling layers and use them to perform the final classification or
regression task.

Before entering the first fully connected layer, the feature maps will typically be flattened
into a one-dimensional vector of values, which will align with the number of input nodes of
the ANN.

H Output layer
The output layer from the CNN is the output that comes from the fully connected layers of
the ANN.

(;Top tip!
Don't neglect transfer learning to leverage pre-trained models. This is especially useful if you have
limited training data available.
B Explore some of the pre-trained models available through TensorFlow or PyTorch, such as
VGG, ResNet and Inception.
m Consider the similarity between the data the model was originally trained on, and your
target data.
B Decide whether you will be using transfer learning to assist with feature extraction or finetuning. Feature extraction will use the previous network to extract meaningful features from
new samples. In this case, freeze the convolutional base and only train a new classifier layer.
Fine-tuning doesn't freeze the convolutional base. After adding your new output layer, it will
fine-tune the weights of the pre-trained model by continuing the training process, allowing it
to learn new task-specific features.
B Remember to preprocess data in the same way the original model was trained, and to use
data augmentation techniques (like rotation, scaling, cropping and flipping) to artificially
expand the training data set.

A4 Machine learning

B Example: CIFAR-10
The CIFAR-10 data set contains 50,000 images of 32x32 pixels in RGB colour, plus another
10,000 test images. The images are labelled over ten categories: airplane, automobile, bird, cat,

=<

deer, dog, frog, horse, ship, truck.
The additional complexity of shapes, slightly enlarged size and the inclusion of three colour
channels rather than just grayscale make CIFAR a good platform for experimenting with
a CNN.
More information about the data set can be found by searching online for Keras CIFARI0 small

images classification data set (https:/keras.io/api/datasets/cifar10).

(®Tok
Is it acceptable to benefit from knowledge derived from unethical sources?
CIFAR-10 is a data set that was compiled by Alex Krizhevsky in 2009. It was created as a subset of a
larger data set of 80 million images known as Tiny Images.
In June 2020, the decision was made to withdraw the Tiny Images data set and request others to
stop using it ,due to “biases, offensive and prejudicial images, and derogatory terminology” within
the data set.
CIFAR-10 remains available and is commonly used by many academic institutions. Students use it to
learn how to train neural networks for computer vision tasks.
Given its origins from the ethically compromised Tiny Images data set, should CIFAR-10 still be used
in scientific research and technological development?

(®Tok
Is it ethical to use data scraping for creating data sets without the consent of the
content owners?
Web scraping is a prevalent tool used to gather vast amounts of data from across the internet.
Typically, this occurs without the explicit consent of the owners or creators of the data.
Web scraping is fundamental to the data sets used for many machine learning models, including
computer vision and large language models.
Courts and governments are grappling with the complex ethical issues around this practice, and
no clear resolution is in sight. There are vast economic and commercial interests on both sides of
the debate.
Some questions include:
B Who owns the information available on the internet, especially on forums such as Reddit, or
collaborative efforts such as Wikipedia?
B Is it ethical to use this data for academic purposes? What about for commercial purposes?
B s it an invasion of privacy to use photos and videos uploaded to social media to form data sets
for machine learning purposes?
B Isit too late? Is it time to focus on harm mitigation? Are there ways to share profits, such as
through royalty payments?

Ad .3 Machine learning approaches (HL)

=
o
Z
25

@

AINO TH

PYTHON
from tensorflow.keras.datasets import cifarl0
from tensorflow.keras import layers, models
import numpy a s np
# Load the CIFAR-10 data set

(train images,

train labels),

(test images,

test labels)

= cifarl0.lcad dataf()

# Normalize the data to 0-1 ranges

train images = train images/255.0
test images = test_images/255.0
# Define the CNN model
model = models.Sequential
([
layers.Conv2D
(32,

(3,

3),

# 32 x 32 pixels,
activation="relu",

input shape=(32,

3 colours
32,

3)),

layers.MaxPooling2D(
(2, 2)),
layers.Conv2D
(64,

(3,

3},

activation="relu"},

layers.MaxPooling2D(
(2, 2}),
layers.Conv2D
(64,

(3,

3},

activation="relu")},

layers.Flatten(),
layers.Dense
(64, activation="relu"),
layers.Dense
(10, activation="softmax")

1)
# Compile and train the model

model .compile (optimizer="adam",

loss='"sparse categorical crossentropy",

metrics=["accuracy"])

N

N

history = model.fit(train images, train_labels, epochs=10, validation split=0.1)
# Evaluate the model
(test images, test labels)
test loss, test acc = model.evaluate
print ("Test accuracy:",

(‘ Top tip!
Start simple! Begin
with simple models to
establish a baseline,
and gradually move
to more complex
algorithms. Appreciate
the power of simple
maodels; sometimes
they are all you need.

test_acc)

A4.3.10 Model selection
You have looked at a lot of machine learning algorithms in this chapter. When the time comes
to use machine learning to solve your own problems, how do you decide which model to use?
Here are some criteria to assist in your decision-making process:
Classification or regression? Does the problem require predicting a continuous output
(regression) or categorizing data into predefined classes (classification)?
Linear or non-linear relationship? Linear regression 1s quick and simple to implement, but
will not work with complex non-linear data, in which case a neural network may be required.
Low or high dimensionality? How many features do you need your algorithm to process?
Volume of data? Deep learning requires a large amount of data to work accurately and to
avoid overfitting. Decision trees or k-nearest neighbours may be better suited if the data set
is small.
Feature independence? If features (variables) interact with each other (such as co-dependency),

the complex interplay may be better captured by a decision tree or neural network.
Accuracy? If highly accurate predictions are required, then more complex models may be the
better option, but this comes at the cost of requiring more data and computational power.

A4 Machine learning

Training time? Linear regression and shallow decision trees can be trained very quickly,
relative to deep neural networks.
Thinking skills: As

Transparency? Sometimes the “magic-happens-here” approach of neural networks may be

a class, brainstorm

a set of case-study
scenarios where
machine learning
may be beneficial.
Debate and discuss
what the appropriate
machine learning
algorithm would
be (for example
linear regression,

clustering,
association

intimidating and undesired by the client. Some domains, such as healthcare or finance,
may require models that can be user-interpreted, in which case linear regression or

decision trees may be best.
Resources available? Deep neural networks require significant GPU computational power
to train. If all you have is a consumer-grade laptop, a simpler approach may be required.

(;Key information
Remember that, as far as the syllabus is concerned, Machine Leamning is a theory unit rather than
a programming one. The following exercises are optional suggestions for students who wish to

explore machine learning programming for themselves.

rules) based on
the problem
statement and data
characteristics.

1

Height and weight (linear regression)
Given the height of a person, can you predict their weight?
Data set @ www.kaggle.com/datasets/galserge/weight-and-height-from-nhanes

2

Vide ogame sales with ratings (linear regression)
Given the ratings assigned by critics reviewing a new Video game, can you predict how many
millions of units a Video game will sell?

Data set @ www.kaggle.com/datasets/rush4ratio/video-game-sales-with-ratings
3

Societal impact on education (linear regression)
How much is a student’s educational outcome influenced by the strength of the economy

and health of the saciety in which they reside?
Data set @ www.kaggle.com/datasets/walassetomaz/pisa-results-2000-2022-economicsand-education
4

Zoo animal classification (k-nearest neighbours)
Classify animals into categories, such as mammal, bird or reptile, based on attributes such as
weight, height and type of habitat.
Data set @ www.kaggle.com/datasets/uciml/zoo-animal-classification

5

Mall customer segmentation (unsupervised k-means clustering)
Use k-means clustering to identify distinct customer groups, such as high-income-highspending vs low-income-high frequency customers.
Data set @ www.kaggle.com/datasets/vjchoudhary7/customer-segmentation-tutorial-inpython

6

Social network analysis (unsupervised spectral clustering)
Discover socially connected communities within the Zachary karate club data set. One

challenge often asked with this data set is to find the two groups of people into which the
karate club split after an argument between two of the teachers.
Search "Zachary karate club” to find this data set.
7

(Unsupervised association rule learning)
Analyse the grocery-store data set to discover common product combinations
purchased together.

Data set @ https://archive.ics.uci.edu/dataset/611

Ad .3 Machine learning approaches (HL)

o
=

7

-

8

(Reinforcement learning)
The previously mentioned Gymnasium has a number of pre-built environments for you to
experiment with.

Download @ https://gymnasium.farama.org
Optimal stock portfolio (genetic algorithm)
Use a genetic algorithm to determine what would be the optimal mix of stocks to hold over
the duration of a data set to maximize return while minimizing risk. The fitness function could
be based on the Sharpe ratio, a measure of return adjusted for risk.
Download historical price data for a set of assets (for example stocks, bonds, ETFs) and
calculate returns for each asset to use in the optimization.
Data set @ www.kaggle.com/datasets/jacksoncrow/stock-market-dataset

Data set 2 @ www.nasdaq.com/market-activity/quotes/historical
10 Stock-price prediction (artificial neural network)
Can you create an Al to accurately predict the performance of stock prices? (If you can, don't
forget to express your appreciation benevolently to the textbook authors &)
Data set @ www.kaggle.com/datasets/jacksoncrow/stock-market-dataset
Data set 2 @ www.nasdaq.com/market-activity/quotes/historical
1" Cats and dogs (convolutional neural network)
Can you tell the difference between a cat and a dog?
Data set @ www.kaggle.com/datasets/shaunthesheep/microsoft-catsvsdogs-dataset
12 Traffic-sign recognition (convolutional neural network)
Accurately detecting road signs is a core challenge for the development of self-driving cars.
The traffic-sign recognition data set contains over 50,000 images across 40 classes of road
sign. Should we let you develop the Al for the next breed of self-driving cars?
Data set @ www.kaggle.com/datasets/meowmeowmeowmeowmeow/gtsrb-german-traffic-sign
13 Movie-reviews sentiment analysis (choose between ANN and CNN)
This exercise will introduce you to natural language processing. Specifically, you will use
sentiment analysis to predict positive and negative reviews based on movie reviews on IMDb.
Data set @ www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews

Be aware that this exercise will involve learning several additional important concepts to
implement. This is because the data set is text, but machine learning models require numeric
data to function, so significant preparation and preprocessing of your data is required.
The following tips will guide you:
O

Convert text to lowercase and remove non-alphabetic characters.

[0 Tokenize the words, which is the process of splitting the text into individual words or
word parts. For instance, the string "hello world" would be tokenized into ["hello",
"world"]. Refer to www.nltk.org/api/nltk.tokenize.html|
[0 Remove stop words, which are words that generally don't convey meaning. Examples
include “a”, “the” and “and"”. Refer to https://pythonspot.com/nltk-stop-words
[ Use a vectorizer such as CountVectorizer or TfidVectorizer to transform your text into
numeric vectors. Refer to https://scikit-learn.org/stable/modules/feature_extraction.
html#text-feature-extraction
O Convert your vectors into NumPy arrays.
[0

Create TensorFlow data sets from the NumPy arrays. Refer to www.tensorflow.org/api_

docs/python/tf/data/Dataset
Now you can build a neural network model and train it.

A4 Machine learning

1

An e-commerce company uses linear regression to predict customer spending based on their
past purchasing behaviour.

2

a

State the assumption about the relationship between the dependent and independent
variables in linear regression.

b

Describe how outliers could affect the performance of the linear regression model in
this scenario.

¢

Describe one method to evaluate the accuracy of this linear regression model.

Areal-estate company uses linear regression to estimate property prices based on features

like area, age and number of rooms. One of the technical staff expressed concern that
multicollinearity might be a problem with the model. Multicollinearity is when two or more
independent variables have a high correlation with one ancther in a regression.

3

a

Explain why multicollinearity might be a problem in this linear regression model.

b

Outline a method to handle multicollinearity if it is found in the data set.

A college uses linear regression to predict student success based on high-school GPA,

standardized test scores and college entrance essays.

4

5

6

a

Outline one reason why it is important to assume linearity in this regression model.

b

Suggest a technique to assess the model's predictive accuracy and explain its importance.

A medical research institution develops a decision tree model to classify patients into risk
categories for heart disease based on lifestyle and genetic data.
a

Describe one advantage of using decision trees for this type of classification problem.

b

Describe one disadvantage of using decision trees for this type of classification problem.

¢

i

Identify one critical parameter in decision trees that could impact the
model’s performance.

il

Qutline its role.

An online retailer uses k-nearest neighbours (KNN) to classify customer reviews as positive,
neutral or negative.
a

Outline how the choice of k affects the classification accuracy in KNN.

b

Describe one method to determine the optimal k value for this application.

¢

Describe how the scales used by features influence the performance of the
KNN algorithm.

A high school wants to classify students into different learning groups based on their learning

styles and previous academic performance.

7

a

Qutline two reasons to select decision trees over KNN for this problem.

b

Qutline two reasons to select KNN over decision trees for this problem.

¢

The school decided to use a decision tree. Describe one strategy to prevent overfitting in
the decision tree model.

A maintenance system uses supervised learning to forecast equipment failures in an
industrial plant.
a

Define "precision” and “recall” in the context of this predictive system.

b

Explain why the F1 score is a better measure than accuracy in scenarios where false
negatives have higher costs.

¢

Describe how a confusion matrix can be used to visually illustrate the success of
the model.

Ad .3 Machine learning approaches (HL)

8

9

A health diagnostic application uses supervised learning to classify patient results as “normal”
or "abnormal”.
a

Qutline the importance of a high recall rate in this medical classification task.

b

Describe how an imbalanced data set might affect the performance metrics like precision
and recall.

¢

i

Identify one method to adjust the classification threshold.

ii

Describe its impact on the F1 score.

An online retailer uses k-means clustering to segment customers based on purchasing patterns.

a

OQutline the objective of the k-means clustering algorithm.

b

Describe one challenge when using k-means clustering for customer segmentation.

¢

Describe how the choice of k affects the outcomes of the k-means algorithm.

10 A telecommunications company uses spectral clustering to segment customers based on
usage patterns.

a

Describe the difference between k-means and spectral clustering in handling nonspherical data clusters.

b

Describe one challenge in using spectral clustering for large data sets.

¢

Describe how the results of spectral clustering could be used to improve
customer satisfaction.

11 A social-media company uses clustering to identify social groups on its network system.
a

Identify which clustering algorithm would allow identification of sodial groups in
this network.

b

Describe one potential challenge in clustering users based on such diverse data.

¢

Describe how the choice of the number of clusters can affect the results.

12 Urban planners in a large city are using data collected from traffic sensors at various
intersections and highways to identify clusters of intersections and road segments that exhibit

similar traffic patterns.
a

Describe a suitable algorithm that the urban planners could use to group sensor data into
clusters based on their traffic characteristics. Explain why this algorithm is appropriate for
handling data with varying densities and noise.

b

Describe how understanding these traffic clusters could benefit the city's traffic
management and infrastructure planning.

13 An e-commerce platform analyses user purchasing data to discover frequent buying patterns.
a

Define “lift" in the context of association rule mining and its importance.

b

Describe how minimum support and confidence levels affect the rules generated in
this scenario.

¢

Describe the potential impact of these buying patterns on targeted marketing strategies.

14 A library analyses borrowing patterns to find associations between different genres of books

borrowed together.
a

Define “confidence” and “support” in association rule mining for this library data.

b

Describe how the library can discover these patterns.

¢

Describe one potential limitation of association rule mining in predicting book-borrowing
patterns.

15 An engineering firm uses genetic algorithms to optimize the design of a new aerodynamic
vehicle model.
a

Outline the role of crossover in genetic algorithms.

b

Outline how mutation affects the evolution process in genetic algorithms.

¢

Qutline one advantage of using genetic algorithms in complex optimization problems
such as vehicle design.

A4 Machine learning

16 A school runs an elective block on the timetable where students can select from a number
of creative and optional courses. Students are asked to indicate their preferred courses but
are not guaranteed to receive their first preference. The school uses a genetic algorithm to
maximize the number of students receiving their first or second preference.
a

Outline the function of selection in genetic algorithms.

b

Describe the concept of “fitness function” in genetic algorithms, and how it might be
applied in this scenario.

¢

Describe how population size influences the outcome of a genetic algorithm.

d

Qutline two benefits of using genetic algorithms to design a timetable schedule.

e

Qutline two drawbacks of using genetic algorithms to design a timetable schedule.

17 A financial institution employs an artificial neural network to predict loan default risk based
on customer profiles.
a

Identify one type of layer often used in neural networks and its purpose.

b

Describe why overfitting might be a concern in neural networks.

¢

Describe how a neural network can be trained to minimize prediction error in this
financial context.

18 An energy company uses a neural network to forecast electricity demand based on weather

conditions and historical usage.
a

i

Identify whether the neural network in this scenario would be regression or
classification based.

i

Qutline the significance of that on its design.

b

Outline one type of activation function used in neural networks and its purpose.

¢

Outline which activation function would most likely be suitable for the output layer in this
scenario, and why.

d

Describe why deep neural networks might be more effective than shallow networks for

this forecasting task.
e

Define “backpropagation” and outline its role in learning within a neural network.

f

OQutline two challenges associated with training deep neural networks.

19 A tech company experiments with several machine learning models to predict user
engagement on a new app.
a

Define “model selection” in the context of machine learning.

b

Identify two metrics that could be used to select the best model for predicting

¢

i

Qutline the concept of cross-validation.

ii

Describe one reason why it is important in model selection.

user engagement.

20 A sports analytics company tests multiple models to predict the outcome of basketball games.
a

Describe the concept of "overfitting” in the context of model selection.

b

Identify three factors that should be considered in model selection.

Ad .3 Machine learning approaches (HL)

Ethical considerations

SYLLABUS CONTENT
By the end of this chapter, you should be able to:
» A4.4.1 Discuss the ethical implications of machine learning in real-world scenarios
» A4.4.2 Discuss ethics as technologies become integrated into daily life

A4.4.1 Ethical implications
(®Tok
Does all knowledge impose ethical obligations on those who know it?
Discuss the ethical use of machine learning, especially in sensitive areas like surveillance or
decision-making.
In surveillance (like facial recognition), concerns about privacy, consent and surveillance biases abound.
Surveillance systems can be used to monitor, control and sometimes discriminate against populations.
With the involvement of machine learning in decision-making, such as in hiring, lending and law
enforcement, these systems can influence people’s lives significantly, and have been shown to
inherit and amplify biases present in the training data.
Machine learning models — inherently knowledge-driven systems — are based on data that
encapsulate various forms of knowledge, from human behaviour to biological patterns. As creators
and users of these systems, there is a responsibility to ensure that this knowledge is used ethically.

The world is changing rapidly. Advances in technology, including those in machine learning,
pose significant challenges and questions for us as a society. It is important to take some time

to weigh these ethical questions and not get caught up by the shiny new tech without taking
the time to think through how it will impact us and those around us. The following are some

of the ethical issues to consider.
B Accountability: With whom or where does responsibility lie for decisions made by machine

learning systems? Is it with the company that produced the AT or the people using it? Is it a
blend of both? Is it possible to determine how and why a machine learning system made a

particular decision?
One incident that highlights the issue of accountability involved a self-driving car. The driver
who was behind the wheel of a self-driving car when it hit and killed a pedestrian in 2018

pleaded guilty to endangerment and was sentenced to three years of supervised probation.
B Algorithmic fairness and bias: Machine learning can perpetuate existing social bias if it

is present in the training data, or if the model's design knowingly or unknowingly favours
certain groups. Fairness requires actively identifying and mitigating bias in the data set

and algorithms.
COMPAS is a recidivism algorithm used by many US court systems. It has been found to

have racial bias, predicting higher risk of recidivism for black people and lower risk of
recidivism for white people.
A4 Machine learning

Another example is that, in 2018, Amazon scrapped a “secret” Al recruiting tool that was
biased against women.
Finally, generative Als have a constant challenge regarding reinforcing and exacerbating

stereotypes and bias.
B

Consent: Large data sets used for training regularly contain information collected without
explicit consent. Many large companies are now performing machine learning on their
customer databases, or selling their customer data to other data-matching companies. How
much control should people retain over their personal information?
Google’s DeepMind was found to be in breach of UK privacy laws after it failed to
adequately inform patients about the use of their personally identifiable health data in
developing an app to detect kidney injuries.

B Environmental impact: Machine learning models require enormous computational
power, especially in the training phase. This leads to substantial energy consumption and
implications for carbon emissions.
Cornell University scientists found that training LLMs (large language models) like GPT3 consumed an amount of electricity equivalent to 500 metric tons of carbon. In fact,
DatacenterDynamics reports global power use by data centres will more than double from

460 TWh in 2022 to over 1000 TWh in 2026.
B Privacy: Machine learning systems can predict or classify personal behaviour in ways
that invade personal privacy. The capacity of machine learning systems to apply inference
means privacy may be further compromised by systems deducing health conditions not
even provided to the model.
In 2018, fitness tracking app Strava released a global heat map of user activities that
inadvertently revealed the locations of secret military bases and patrol routes, showcasing a
significant privacy leak.
B Security: Machine learning systems can be vulnerable to attack through a variety of means.

Three common attacks include:
[0 data poisoning, which involves introducing untrue or harmful data into the training
data set to manipulate the model for nefarious purposes
0 model evasion, where input (such as prompts) is used to “trick” the model into making
incorrect outputs against its training (sometimes known as “jailbreaking”, in the
context of generative AI)

0 model inversion, referring to gaining access to sensitive data contained within the

training data.
‘Within 24 hours of release, Microsoft's Tay Twitter Bot was manipulated through malicious

input data to produce grossly inappropriate and offensive tweets.
‘When GPT-3 was first released by OpenAl, it lacked many of the filters now present and it
was trivially easy to engineer prompts that produced foul, toxic or illegal content.
B Societal impact: Machine learning is increasingly disrupting employment markets, and
influencing public opinion. There is a careful balance between technological advancement

and maintaining social welfare, which needs to be considered.
Clearview Al, which scrapes billions of photos from the internet for facial recognition, has
raised societal concerns about surveillance, consent and civil liberties.

m Transparency: Most engineers cannot explain how their systems generate the outputs
they create, especially those that use neural networks. The best that can be done is to

A4
.4 Ethical considerations

-

point to the training data rather than the algorithm itself. This lack of transparency,
or human understandability, of what these algorithms are and how they work poses
significant questions.
In 2019, tech entrepreneur David Heinmeier Hansson wrote on X (formerly Twitter) that

Apple Card offered him 20 times the credit limit of his wife, although they have shared

assets and she has a higher credit score, raising questions about the transparency of the
algorithms used for financial decision-making.
Bias in training data: Bias in training data is a core challenge for machine learning. Overor under-representation of particular demographics will affect the model’s predictions and

reliability. Rigorous data collection, processing and evaluation methods are required to
ensure broad and fair representation.
Misinformation: Machine learning can generate and spread false information with
ease, making it very difficult to ensure accurate and reliable communication online. As
generative Al in particular, becomes increasingly realistic and convincing in its outputs,
it will become almost impossible to avoid falling victim to fake news, fake images and

fake videos.
It is believed that misinformation on Facebook received six times more clicks than factual
news during the 2020 US election, according to a study by NYU.
As generative Al deep fakes become weapons of the political debate, confusion over what to
believe will only pose more complex challenges in the future.
Bias in online communication: Machine learning-based recommendation systems are
designed to maximize user engagement on a platform. One method of doing this is by
recommending more of the same kinds of content that users have previously engaged
with. This can create “echo chambers” that reinforce existing beliefs and minimize
alternative viewpoints.

Facebook newsfeed algorithms and YouTube’s recommendation systems have both been
criticized for crearing filter bubbles and echo chambers, where users are predominantly

shown content that aligns with their existing beliefs, potentially polarizing public opinion.
Online harassment: Machine learning can be used to automate harassment on an
enormous scale. Bots can troll and target individuals or groups with ease, and can
increasingly make it seem like the attacks are coming from people. Generative Al is being

used to create deep fakes in hurtful and abusive ways that authorities are struggling to keep
up with.
Privacy and anonymity in online communications: Users often are not aware of or do not
fully understand how their data are used and processed by machine learning algorithms.

Users may think their actions are anonymous, but increasingly machine learning
algorithms can perform de-anonymization with a high degree of reliability. There is very

little awareness of this in the broader community.
In 2006, Netflix released a data set containing 100 million movie ratings from 500,000
subscribers, intended for use in a global competition to improve the accuracy of Netflix’s

recommendation algorithm. The data was supposedly anonymized by removing any personal
identifying information. Researchers from the University of Texas at Austin demonstrated
that it was possible to re-identify users by comparing the anonymized Netflix data with
publicly available movie ratings on the Internet Movie Database (IMDb). Using only a small
amount of additional information about an individual’s preferences, the researchers were able
to identify personal viewing habits and potentially sensitive information.

A4 Machine learning

A4.4.2 Reassessing ethics as technologies

become further integrated
As s artificial intelligence and other technology continues to advance and evolve over the years
ahead, society is going to need to regularly reassess the implications from an ethical viewpoint.
T here are many challenges that lie ahead; the following list is just a discussion starter.
u

Quantum computing: Quantum computing could potentially break many of the
cryptographic systems that currently secure digital communications and cryptocurrencies.
The development of quantum-resistant cryptography is an important area of research that
needs to be prioritized.

Augmented reality: AR can collect vast amounts of personal data about users’
environments. Additionally, what are the ethics around altering a person’s perception of

reality? Does this disconnect them from the society of which they are part, resulting in a
loss of empathy?
Virtual reality: As VR becomes more realistic, what are the mental-health concerns for
those who use the systems excessively or for escapism? What should the limits be when it

comes to VR being used to access violent or explicit material?
Pervasive AL: How do we guard against intrusive surveillance and the seemingly neverending collection of our personal data for use in machine learning data sets?
Privacy: Who owns the data about you? Is it you, or the company that collected it? As data
collection becomes more complex, will there be a move towards more transparent and

informed consent about what happens with our personal information?
Equity: How can we ensure that advances in technology reduce rather than magnify
equitable access to technology across socio-economic, racial, gender, social and
geographical groups?

(;Top tip!
This section shared
real-life case studies
on the impact of
many of the ethical
questions being
raised by this topic.
Be familiar with case
studies that you can
refer to in your exam
responses. If you can
discuss with specificity
a relevant situation
that occurred, it goes
a long way towards
demonstrating that
you care about
the issue.

GCommon mistakes

A4
.4 Ethical considerations

Students make a number of common errors when addressing ethics-related questions, which
extends to the discussion of machine learning.

m Don’t oversimplify the issues. Avoid reducing complex ethical issues to simple right or
wrong answers. The ethical implications of machine learning are nuanced and often involve
interconnected considerations of accountability, fairness and societal impact.

® Don’t confuse technical bias with ethical bias. Distinguish between technical bias
(deviation in an algorithm that leads to less accurate predictions) and ethical / social bias
(prejudices in data that lead to unfair outcomes for certain groups).
® Don’t limit your responses to issues of privacy and security. Consider a broader range
of ethical issues, such as environmental impact, sodietal changes and the implications for
mental health. Show you have a deep understanding of the complexities involved, rather
than taking the lazy approach of resorting to an exam response that discusses privacy or
security superficially.
B Don’t neglect the importance of reassessment. Ethics guidelines can never be static, as
technology and its impact on society is not static.

Social skills: Set up a class debate or panel discussion where you argue the ethical
implications of using machine learning, such as bias, privacy and transparency concerns.
Facilitate peer feedback sessions where you review and provide constructive criticism on

each other’s machine learning projects or presentations.
Some possible debate prompts include:
B Should health-insurance companies have access to predictions about potential future
ilinesses to set premiums, even if this could lead to higher costs for those deemed at
higher risk?
B Should autonomous vehicles be programmed to prioritize the lives of pedestrians
over the life of the vehicle’s passenger(s)? How should these ethical decisions be
programmed into autonomous systems?
B s it ethical to use a recruitment tool that shows bias towards certain educational
institutions? Should the company stop using it until it can be proven to be unbiased?

m If a city implements widespread facial recognition through CCTV cameras to reduce
crime, is this worth the lessening of privacy or the risk of false accusation?
B Should social-media platforms be held responsible for breaking echo chambers and
ensuring a balanced exposure to different viewpoints? How can this be balanced with
business models that require maximizing engagement to earn revenue?

1

An Al company develops a facial recognition system used in public surveillance.
Outline three ethical implications of using facial recognition technology in public spaces.
b
¢

2

3

i

Identify two potential biases that could arise in facial recognition systems.

it

Outline the societal impacts of each.

Outline two measures that could be implemented to address these ethical concerns
and biases.

Asocial-media company uses algorithms to personalize newsfeeds based on user interactions.
a

Qutline two potential ethical issues related to algorithmic bias in personalizing newsfeeds.

b

Qutline two strategies the company could implement to ensure the ethical use of
personalization algorithms.

¢

i

Identify two implications of lack of transparency in algorithmic decision-making.

il

Outline two methods to improve transparency in algorithmic decision-making.

A university uses Al to make admissions decisions based on application materials.
a

¢

Outline three potential ethical concerns with using Al in university admissions.
i

Outline two possible biases that could arise in this Al system.

il

Outline their impact on students.

Describe measures to address these ethical concerns and biases.

sesssssssssssnnnsanns

@ Linking questions
1
2

How can machine learning be applied to optimize network traffic management? (A2)
How does database programming in SQL differ from programming computationally in a
high-level language? (A3, B2)

3

To what extent are developments in machine learning ethical? (TOK)

4
5

How can larger models be processed using GPUs and cloud processing? (A1)
Can machine learning find and improve network security problems? (A2)

D

L

T

P

Sssssssssssssssansassnnas

D
R P T

T

A4 Machine learning

1

Health monitoring app
A tech startup has developed a health monitoring app that uses machine learning to predict
potential health issues based on user-inputted symptoms, lifestyle data and historical health data.
The app classifies user health into categories such as “low risk”, “medium risk” and "high risk".
a i State whether this system should be classified as artificial intelligence or
machine learning.
[1]
il Outline one reason for your choice.
2]
b Describe the potential need for specialized hardware (e.g. GPUs) in deploying this app
on mobile devices.
2]
¢

Describe the importance of data cleaning in this scenario, particularly addressing

d

missing values in lifestyle data.
Describe how feature selection could impact the accuracy and efficiency of the
predictive model used in the app.

e

Suggest the type of machine learning algorithm that would be suitable for this
classification task.

2]
2]
(4]

f

2

3

Outline the implications of choosing a high value of & in a k-nearest neighbours (KNN)
algorithm for this application.
2]
g Outline three ethical concerns related to privacy and data security in health-related apps.
[3]
h
Describe two measures that could be implemented to address potential biases in the
data set, especially relating to underrepresented groups.
2]
Autonomous public transport system
A city plans to implement an autonomous bus service that uses machine learning to optimize
routes based on traffic patterns, weather conditions and passenger demand.
a i Define "edge computing”.
[1]
ii
Describe its relevance in real-time data processing for autonomous vehicles.
2]
b Describe whether a deep learning model would be more effective than a traditional
machine learning model for processing complex environmental data.
2]
¢ Describe how data normalization affects the performance of machine learning models
dealing with varied data types such as weather conditions and traffic density.
2]
d i Identify a common data quality issue that might arise with real-time traffic data.
[1]
ii Outline a preprocessing step to mitigate these issues.
2]
e Describe how reinforcement learning could be applied to optimize bus routes dynamically. [2]
f Discuss the potential use of transfer learning from other cities’ traffic management
systems to improve route optimization.
2]
g Describe the ethical implications of using surveillance data (e.g. from traffic cameras)
in training machine learning models for public transport systems.
2]
h Describe the societal impacts of replacing human-driven buses with autonomous
buses, including job displacement and public safety.
2]
Al-powered recruitment tool
A multinational corporation implements an Al-powered tool to screen job applications and predict

the suitability of candidates based on their résumés and answers to pre-interview questions.
a Describe the classification vs regression nature of the predictive model used by the
Al tool.
b Describe the impact of processing speed and memory requirements on the scalability
of the Al tool across the corporation’s global offices.
¢ i Identify potential biases in the training data set.
i Outline how these could be mitigated during data preprocessing.
d Describe the importance of feature selection in improving the predictive accuracy of
the Al tool.
e Describe the use of a decision tree model over a regression-based model for this
classification task.

A4
.4 Ethical considerations

2]
2]
[1]
2]
2]
[3]

f

4

Outline two ethical concerns related to Al decision-making in recruitment, particularly
in terms of fairness and transparency.
[2]
g Describe two methods to ensure the ethical use of Al in recruitment with respect to
increasing transparency and accountability.
2]
Retail customer segmentation
A large retail chain uses machine learning to segment its customer base to personalize
marketing strategies and improve customer service.

a

5

Discuss whether supervised or unsupervised learning is more appropriate for
customer segmentation.
4]
b Outline the potential benefits of using cloud computing resources over in-house
servers for processing large customer data sets.
[2]
¢ Outline the role of outlier detection in customer segmentation.
2]
d i
Identify a clustering algorithm suitable for handling large data sets with
high dimensionality.
[1]
il Describe a reason for your choice.
2]
e Describe the potential privacy issues that may arise from the detailed segmentation of
customers’ buying habits.
[3]
f Describe strategies to mitigate the risk of discriminatory marketing practices that could
result from biased data in customer segmentation.
2]
Natural-disaster prediction and management
A government agency deploys machine learning models to predict natural disasters such as
floods and earthquakes, aiming to enhance preparedness and response strategies.

a

Describe the application of neural networks in predicting natural disasters and the
kind of data they might process.
Describe the impact of using real-time data processing on system requirements
and infrastructure.

[2]

¢

Describe the challenges associated with integrating and cleaning data from multiple
sources, such as satellite imagery and geological sensors.

2]

d

Describe the role of data augmentation in improving the accuracy of predictions in

b

areas with sparse historical data.

2]

e

Describe the use of deep learning over traditional models for predicting complex
natural disaster patterns.

[2]

f

Describe how machine learning models can be trained to adapt to new types of

disaster data over time.
Describe the ethical implications of false positives and false negatives in disaster

2]

g

prediction models.

[3]

Describe protocols for data governance that may ensure sensitive geographical and
personal data used in predictions are protected.

[2]

h
6

2]

Automated cyberbullying detection system
A software company is developing an automated system to detect and flag instances of

cyberbullying on social-media platforms using natural language processing and machine learning.
a i
Define “natural language processing (NLP)".
[1]
il Describe its relevance in detecting cyberbullying.
[2]
b i
Describe the computational challenges associated with processing large volumes
of social-media data in real time.
2]
il Outline appropriate hardware solutions.
[2]
¢ Describe the potential preprocessing steps needed for textual data from social-media
posts to prepare it for machine learning models.
2]
d Explain the importance of handling sarcasm and ambiguities in text when setting up
preprocessing pipelines for detecting cyberbullying.
[2]
e Describe the use of analytical rule-based systems vs machine learning models in the
context of cyberbullying detection.
[2]
f
OQutline the ethical considerations of implementing an automated cyberbullying
detection system, particularly regarding false positives and false negatives.
[3]
g Describe the potential privacy implications of analysing users’ social-media content,
even for the purpose of detecting cyberbullying.
[2]

A4 Machine learning

B1 Computational
thinking

Approaches to
computational thinking
How can we apply a computational solution to a real-world problem?

SYLLABUS CONTENT
By the end of this chapter, you should be able to:
» B1.1.1 Construct a problem specification
» B1.1.2 Describe the fundamental concepts of computational thinking
» B1.1.3 Explain how the fundamental concepts of computational thinking are used to
approach and solve problems in Computer Science

P B1.1.4 Trace flowcharts for a range of programming algorithms

B1.1.1 Problem specification
Ever since their beginnings, computers have required a method to instruct them to perform
a specific task. Now, we provide instructions to a computer via a programming language.

Ada Lovelace, Charles Babbage, Alan Turing and Konrad Zuse are all recognized for their
contributions to the development of coding and computer languages. Initially, programming

languages were developed as a series of steps to wire a particular program. Then, they
developed into a series of steps typed into a computer and then executed. Later, they acquired
4 Problem
specification: a short,
clear explanation of
an issue, which may
include: a problem
statement; constraints

more advanced features, such as iterations; branching and even polymorphism; inheritance;
and other object-oriented programming principles.

Even when tackling straightforward problems, it is essential to furnish the computer with
precise instructions to enable it to carry out the tasks and resolve the problem.
However, you will not be able to provide clear instructions on how to solve a problem until you

and limitations;
objectives and goals;
input and output

A problem specification is a short, clear explanation of an issue, outlining who the

specifications; and

stakeholders are and why it is important to solve the problem. The problem specification

evaluation criteria.

may include a problem statement; constraints and limitations; objectives and goals; input and

4 Stakeholder: an

output specifications; and evaluation criteria.

individual or group(s)

of people within or

clearly outline the problem specifications.

This is a great opportunity to think of your internal assessment project. When you define the

outside an organization

problem statement, you need to include a description of the problem itself, who the solution

who are affected
or think they are
affected by a software
development project.

is designed for, the issues encountered and what needs to be solved. To clearly understand

# Problem statement:
a description of
the problem itself,
identification of who the
solution is designed for,

the issues encountered
and what needs to
be solved.

the problem, you are encouraged to collect information from existing literature and research,
use previous experiences with the problem and discuss it with multiple stakeholders who are

impacted by the problem. In this way, you will be able to identify some possible constraints
and limitartions, for example:
B Limitations regarding the available technical requirements (hardware or software equipment)

Economic aspect (cost of producing the solution)
Legislation (regulations regarding the software development; ethical, social and legal aspects)
Operational issues (workforce available)
Schedule (time required to develop and implement the solution).
A4 Machine learning

Each question in section A will focus on content from one unit (sometimes linking
to another unit). The questions will be presented in the following format.
*

First, a scenario: a short introduction to a context in which the question paper
is being asked.

*

There will then follow around two to five questions based on the scenario.
For example:
o Question 1: a short question related to the scenario
© Question 2: a slightly harder question related to the scenario
© Question 3: a hard question related to the scenario.

External assessment: Paper 1 and Paper 2

Example question
4

A school stores information about students within a database.

The parent/guardian evening view, MEETING, is an extract showing the
different appointments between students, parents/guardians and teachers.
The time represents the time of the meeting.
Student ID | Student Name | Parent/Guardian | Teacher ID
Name
3948
4944
3399

Colin Stephan | Ray & Barbara
Tessa Allingham | Melanie
Lucy Simons

Rayissa

Teacher

Time

Subject

Name
48
34
23

Dheepa

10:00

Maths

Marta

10:15

Spanish

Dheepa

10:15

Maths

Michael

10:30

German

Ole

10:00

Music

(a) Constructa SQL SELECT query to list the name of students and
parents/guardians who have appointments to meet with
maths teachers.

(3]

The database view, MEETING, can be represented using the following notation:
MEETING(Student ID, StudentName, Parent/GuardianName, TeacherlD,
TeacherName, Time, Subject)

(b) Construct the database in third normal form (3NF) for the three
entities in the MEETING view extract.
The school has multiple people accessing the system at the same time.
Atomicity needs to be applied to the database to ensure no errors occur.

(3]

(c) Describe how atomicity manages transactions within the database.
The school stores data regarding students’ grades and medical issues on
the database.

(d) Outline one ethical issue regarding this data within the database.

(3]

Section A of paper 1 aims to assess your knowledge and understanding as well
as your ability to apply the work you have studied in theme A. There are 38 marks
available on the standard-level paper and 56 marks available on the higher-level
paper.
Remember the following.
All questions are compulsory.
There will be questions testing AOT, AO2 and AO3.
When working through the questions, take your time, read the question
carefully, and look for clues in the question to help you identify what you
need to do. If you rush, you may misinterpret the question.
Consider using ofa highlighter to identify key terms and phrases. For example:
State . reason _ is carried out before- aCNN
Make sure you answer the question in a manner expected by the command
term. A description of these can be found in Table 1.

[1]

External assessment: Paper 1 and Paper 2

*

[fyou find a question that you are unsure how to answer, make a note on the
page and move on to the next question. You can come back to this question
if you have time at the end of the exam.

*

Take time before the end of the exam to review your answers. Fill in any gaps.
There is no negative marking, so you lose nothing from an educated guess!

Paper1 Section B—Case study
Section B of paper1 is based on a previously encountered case study. The case
study is a scenario that allows you to study current developments, emergent
technologies, and ethical issues in computer science, and you will be given class
time to do this. Your teacher may guide the class through the investigation or you
may complete the investigation yourself. Your teacher will receive the case study
in the June before your exam (for both May and November candidates).
The case study allows you to investigate concepts you have learned throughout the
computer science course in a real-world context. Higher-level students will study
four challenges in detail and standard-level students will study two challenges.
By the end of the case study investigation, you should be able to:
*

show an understanding of how the systems in the case study work

*

show an understanding of computational thinking fundamental to the
systems in the case study

*

apply concepts from the course syllabus in the context of the case study

*

explain how scenarios in the case study may be related to other scenarios

¢

discuss the impacts and ethical issues relevant to the case study

*

evaluate, formulate and justify strategies based on the information from
the case study itself, your own research, and new stimulus provided in the
examination paper.

You will be given a clean copy of the case study in the exam. The case study
(section B of the exam) is worth 12 marks at standard level and 24 marks at
higher level.
At standard level, you will be expected to answer around three case study
questions. At higher level, you will be expected to answer four or five shorter
questions and one longer question.
Examples of possible questions about the material in the case study.
Outline one other potential consequence of ...
Outline two ways that a X improves the performance of ...
To what extent do the benefits ofX outweigh the risks of Y ...
Describe two advantages of ...
Describe two strategies that could be used to ...
For the case study section of your exam, you might find it helpful to:
*

highlight key terms that will help you to remember what information to
include in your answer.

*

plan out your answer to longer questions before starting your response.

External assessment: Paper 1 and Paper 2

Paper 2—Theme B: Computational thinking and
problem-solving
There are two options for paper 2, allowing you to complete the paper using the
Java programming language orthe Python programming language. You should
not attempt both! The paper will consist of all the content you have learned in
theme B of the course. This includes the following units:
*

Bl Computational thinking

*

